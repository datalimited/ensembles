```{r, echo=FALSE}
# Values to use in the paper:
load("values.rda") # generated by `../analysis/9-output-values.R`

# Includes: mean_sim, mean_ram, slope_sim, auc_sim, ram_stocks_n

# mean_sim, mean_ram, and slope_sim have the following list elements:
# > names(mean_sim)
# "mach_vs_ind_mare_fold" "ind_corr_range" "mach_corr_range" "ind_mare_range"  
# "mach_mare_range" "ensemble_mre_range" "ind_mre_range"
 
# auc_sim has the following list elements:
# > names(auc_sim)
# "mach_range" "ind_range" 
```

<!--notes:
http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1523-1739/homepage/ForAuthors.html

con bio: American spelling

Contributed Papers: 6000 words

Submit your cover page as a separate document. It should not be part of the
manuscript itself. The cover page must include the title of the paper; a running
head (short title of 40 or fewer characters); a list of 5-8 keywords; word count
(all text from the first word of the Abstract through the last word of the
Literature Cited but not including table or figure legends or the body of
tables); authors' complete mailing addresses (including postal code) at the time
the work was conducted and present addresses if different; name and complete
address (including email) of the person to whom correspondence should be sent;
and text of your acknowledgments section.

Contributed Papers, Research Notes, and Conservation Practice and Policy papers
should contain the following sections in the following order: Abstract,
Introduction, Methods, Results, Discussion, Supporting Information paragraph (if
there are online appendices), Literature Cited, tables, figure legends, and
figures. Do not combine sections (e.g., Results and Discussion). The
Acknowledgments section will be added to the body of the paper after the
manuscript has been accepted. Do not number section headings or subheadings. Do
not include a Conclusion section (conclusions are part of the Discussion). 

You may have color figures in the online version and gray-scale figures in
print. However, reference to color cannot be made in the figure legend or in the
text, and elements in the gray-scale version must be distinguishable. Supply
separate files for color and gray-scale figures.

-->

\noindent
Running head: Superensembles of population status

\bigskip

\begin{Large}
\noindent
Improving estimates of population status and trajectory with superensemble models
\end{Large}

\bigskip

\noindent
Sean C. Anderson^1\*^, 
Andrew B. Cooper^1^,
Olaf P. Jensen^4^, 
C\'{o}il\'{i}n Minto^6^, 
James T. Thorson^11^,
Jessica C. Walsh^1^,
Jamie Afflerbach^2^, 
Mark Dickey-Collas^3,12^, 
Kristin M. Kleisner^5^, 
Catherine Longo^2^, 
Giacomo Chato Osio^7^, 
Dan Ovando^8^, 
Andrew A. Rosenberg^9^, 
Elizabeth R. Selig^10^

\noindent
\textit{order to be determined and superscripts out of order}

\bigskip

\noindent
^1^School of Resource and Environmental Management,
Simon Fraser University, Burnaby, BC, V5A 1S6, Canada

\noindent
^2^National Center for Ecological Analysis and Synthesis

\noindent
^3^International Council for the Exploration of the Sea, 
H.C. Andersens Boulevard 44-46, DK 1553, Copenhagen, Denmark

\noindent
^4^Institute of Marine & Coastal Sciences, Rutgers University, 
71 Dudley Road, New Brunswick, NJ, 08901-8525, USA

\noindent
^5^NOAA/Northeast Fisheries Science Center, Ecosystem Assessment Program, 
166 Water St., Woods Hole, MA, 02543, USA

\noindent
^6^Galway-Mayo Institute of Technology, 
Marine and Freshwater Research Centre, Galway, 00000, Ireland

\noindent
^7^EC JRC, IPSC, MAU, Italy

\noindent
^8^University of California Santa Barbara, 
Bren School of Environmental Science and Management, 
Santa Barbara, CA, 93106-5131, USA

\noindent
^9^Union of Concerned Scientists, Cambridge, MA, USA

\noindent
^10^Conservation International, Arlington, VA, USA

\noindent
^11^National Marine Fisheries Service, 
National Oceanic and Atmospheric Administration, 
Fisheries Resource Assessment and Monitoring Division, 
Northwest Fisheries Science Center, Seattle, WA, 98112, USA

\noindent
^12^DTU Aqua National Institute of Aquatic Resources, Technical University of Denmark (DTU), Jægersborg Alle 1, 2920 Charlottenlund, Denmark

\noindent
^\*^Corresponding author: Sean C. Anderson,
School of Resource and Environmental Management,
Simon Fraser University,
Burnaby BC, V5A 1S6;
E-mail: sean_anderson@sfu.ca

\clearpage

# Abstract

<!--up to 300 words-->

Ecological resource managers are often faced with reconciling multiple
potentially conflicting estimates of population status and trajectory. One
solution is to average these predictions in an ensemble, but this approach
ignores covariance between models and may not optimally reduce bias and improve
accuracy. Superensemble models, commonly used for climate and weather
forecasting, may provide a superior solution. Superensembles use the predictions
from multiple models as covariates in a new statistical model. Here we evaluate
the potential for ensemble and superensemble models ('ensemble methods') to
improve estimates of fish population status and trajectory. We fit four
data-limited models of population and exploitation dynamics to marine fish
stocks and combine their estimates of the mean and slope of biomass at maximum
sustainable yield ($B/B_\mathrm{MSY}$) with an ensemble average and three
superensembles: a linear model, a random forest, and a boosted regression tree. 
We selected data-limited models for their applicability across the majority of
fish stocks world wide. We build our models on a simulated dataset of 5760
stocks and tested them with cross-validation and a global database of 
`r ram_stocks_n` stock assessments that have also been assessed with more
data-intensive stock assessment models. We find that ensemble methods
substantially improve estimates of population status and trajectory.
Machine-learning superensembles performed the best for population status:
accuracy improved
`r mean_sim$mach_vs_ind_mare_fold`%, rank-order correlation between predicted
and true status improved from 
`r mean_sim$ind_corr_range` to `r mean_sim$mach_corr_range`, and bias (median
proportional error) declined from 
`r mean_sim$ind_mre_range` to `r mean_sim$mach_mre_range`. 
We found similar improvements when predicting the trajectory of status and when
applying the simulation-trained superensembles to catch data for global fish
stocks. Ensemble methods have the potential to improve estimates of population
status and trajectory across many taxa beyond fish. However, they must be
carefully tested, formed from a diverse set of accurate models, and built on a
dataset sufficiently representative of the populations to which they are
applied.

\bigskip
\noindent 
Keywords: data-limited fisheries, ensemble methods, multi-model averaging, population dynamics, sustainable resource management
<!--Include on the cover page five to eight words or phrases that will be useful for indexing and literature searches. Do not use words in the title as keywords, and avoid general terms such as conservation.-->

# Introduction

Population status and trajectory are two of the most fundamental values to
quantify in conservation biology [e.g. @mace2008; @iucn2015]. These properties
are key to everything from global assessments of extinction risk [e.g.
@stuart2004; @schipper2008] to assessing the sustainability of hunting and
fishing exploitation levels [e.g. @worm2009; @juan-jorda2011]. However,
conservation and ecological resource managers are often faced with reconciling
multiple uncertain and potentially conflicting estimates of status and
trajectory [e.g. @brodziak2010; @branch2011]. Some models may suggest a
population is at risk and declining in abundance while others may suggest it is
not at risk and stable.

One solution is to take the average or weighted average of an ensemble of model
predictions. Such ensembles are typically more accurate and less biased than
individual model estimates and can integrate uncertainty in model structure,
initial conditions, and parameter estimation [@dietterich2000; @araujo2007].
This approach forms the basis of many machine learning methods [e.g.
@dietterich2000], has helped reconcile climate forecasts from dozens of models
[e.g. @murphy2004; @tebaldi2007; @ipcc2013], and even improved early warning
signs of malaria outbreaks [@thomson2006]. In ecology, ensemble methods are
frequently used to improve species distribution modelling [e.g. @araujo2007;
@breiner2015] and indeed have been used to combine estimates of population
status and trajectory [e.g. @brodziak2010].

Whereas averages or weighted averages may improve predictions over any single
model, they may not optimally leverage available data. The truth does not
necessarily lie in the middle of multiple model predictions, some models may
perform better than others in certain conditions, and the covariance between
models may contain information that can improve predictive accuracy. We can
exploit these characteristics by using the predictions from a group of models as
inputs into a separate statistical model. This technique, sometimes called
superensemble modelling [@krishnamurti1999], is common in climate and weather
forecasting [e.g. @yun2005; @mote2015]. The superensemble is fit to some
training dataset where outcomes are well known and used to predict on a dataset
of interest. For example, @krishnamurti1999 combined predictions of wind and
precipitation in Asian monsoons via a superensemble regression fit to observed
data. Their superensemble was considerably more accurate than any individual
prediction or weighted average of predictions. 

In fisheries science, the commonly used operational model for determining status
and trajectory of exploited fish populations are stock assessments that
incorporate all appropriate data (e.g. catches, size and age distributions,
surveys, and tagging information) to quantify values such as the biomass that
can produce maximum sustainable yield ($B/B_\mathrm{MSY}$). However, a broad
range of data are not available for the majority of fish populations [@fao2014].
Therefore, a number of models have been proposed to assess $B/B_\mathrm{MSY}$
based on the limited data available for the majority of fish stocks: catches and
a basic understanding of population productivity [e.g. @vasconcellos2005;
@martell2013]. Recently, @rosenberg2014 investigated the performance of four
data-limited models through a large-scale simulation experiment. The four models
frequently disagreed about population status and any one model had relatively
poor predictive ability on average (e.g. Fig. \ref{motivate}).                                                     

Here, we estimate population status and trajectory of exploited fish populations
using ensembles and superensembles (collectively 'ensemble methods') of these
four data-limited models. We explore a variety of approaches applied to both
simulated and real-world fish stocks and compare predictive performance. We find
that ensembles, and in particular superensembles, generally improved predictive
performance of status and trajectory of fish populations over any single
model---especially when considered across multiple dimensions of predictive
ability. Furthermore, the output from superensembles can aid mechanistic
understanding of how individual models of status and trajectory perform.

# Methods

<!--
We tested the ability of ensemble models to improve estimates of population
status and trajectory when applied to both a large simulated dataset and a
global database of assessed stock status. Here we describe the datasets,
individual models of population status, and ensemble methods to combine those
estimates. We then describe how we evaluated the ability of the various models
to estimate population status.
-->

<!--
## Datasets
-->

We first developed and tested ensemble methods with a fully factorial simulated
dataset [@rosenberg2014] with precisely known status. These published
simulations included three fish life histories: small pelagic, demersal, and large
pelagic; three levels of how depleted biomass was at the start of the dataset
compared to carrying capacity: 100%, 70%, and 40% of carrying capacity; and four
harvest dynamics: a constant harvest rate, a harvest rate
coupled with biomass to mimic an open-access single-species fishery, a scenario
where harvest rate increases continuously, and a 'roller-coaster'
scenario where the harvest rate increases and then decreases.
Process noise was introduced to the models through two levels of multiplicative
recruitment variability: $N(0, 0.2)$ and $N(0, 0.6)$, and was either
uncorrelated or had a first-order autoregressive correlation of $0.6$. The
simulation included a scenario without observation error and with multiplicative
observation error around catch at $N(0, 0.2)$. 

@rosenberg2014 ran ten iterations for each combination of
factors adding stochastic draws of recruitment and catch-recording variability
each time to generate a total of 5760 stocks. The simulation models were built
in the package `FLR` [@kell2007] for the statistical software \textsf{R}
[@r2015]. The code to generate the simulations is available at
<https://github.com/flr/StockSims> (TODO: not there). 

We also tested our ensemble methods on the RAM Legacy Stock Assessment Database
[@ricard2012]. Our analysis of the stock-assessment database was based on
version XX downloaded on XX. After removing stocks for which the models
described below did not converge, this database included `r ram_stocks_n`
stocks.

## Individual models of population status

We fit four individual data-limited models to estimate $B/B_\mathrm{MSY}$. Three of the
models are mechanistic and based generally on Schaefer biomass dynamics
[@schaefer1954] of the form

$$\hat{B}_{t+1} = B_t + r B_t \left(1 - B_t / B_0 \right) - C_t,$$

\noindent
where $\hat{B}_{t+1}$ represents predicted biomass at time $t$ plus one year,
$B_t$ represents biomass at time $t$, $r$ represents the intrinsic population
growth rate, $B_0$ represents unfished biomass or carrying capacity $K$, and $C$
represents catch. @rosenberg2014 provide a full summary of
these four methods and code to fit all the models is available in an
accompanying package `datalimited` for the statistical software \textsf{R}. In
summary:

* *CMSY* (catch-MSY) implements a stock-reduction analysis with Schaefer biomass
  dynamics [@martell2013]. It requires a prior distribution for $r$ and assigns
  a prior to the relative proportion of biomass at the end compared to unfished
  biomass (depletion) based on the percentage of maximum catch at the end of the
  time series. Our version of the model was modified from @martell2013 to
  generate biomass trends from all viable $r$-$K$ pairs and produce an estimate of
  $B/B_\mathrm{MSY}$ from the median trend.

* *COM-SIR* (catch-only-model with sample-importance-resampling) is a coupled
  harvest-dynamics model [@vasconcellos2005]. Biomass is assumed to follow a
  Schaefer model and harvest dynamics are assumed to follow a logistic model.
  The model is fit with a sample-importance-sampling algorithm [@rosenberg2014].

* *SSCOM* (state-space catch-only model) is a hierarchical model that, similar
  to COM-SIR, is based on a coupled harvest-dynamics model [@thorson2013].
  SSCOM estimates unobserved dynamics in both fishing effort and the fished
  population based on a catch time series and priors on $r$, the maximum rate
  of increase of fishing effort, and the magnitude of various sources of
  stochasticity. The model is fit in a Bayesian state-space framework to
  integrate across three forms of stochasticity: variation in effort,
  population dynamics, and fishing efficiency [@thorson2013].

* *mPRM* (modified panel regression model) is a modified version of the
  panel-regression model from @costello2012. Unlike the other
  models, mPRM is empirical and not mechanistic---it uses the RAM Legacy Stock
  Assessment database to fit a regression model to a series of characteristics
  of the catch time series and stock with stock-assessed $B/B_\mathrm{MSY}$ as
  the response. The model used in this paper is modified from the original in
  that it condenses the life-history categories into three categories to match
  the simulated dataset, removes the maximum catch predictor since the absolute
  catch in the simulated dataset is arbitrary, and does not implement the bias
  correction needed in @costello2012 for deriving aggregate
  estimates of median status across multiple stocks.

## Additional covariates

<!--TODO Kristin and Mark: It might be a good idea to add a figure that
illustrates the spectrum of ensemble/superensembles that are applied here and
what is included (e.g., non-linear relationships, etc.). This will help the
reader understand the progression in complexity of the ensemble approaches I
think.-->
Superensemble models allow us to incorporate additional covariates 
and potentially leverage interactions between these covariates and individual
model predictions. Additional covariates could be, for example, life-history
characteristics, information on exploitation patterns, or statistical
properties of the data. For simplicity, and to allow us to apply models
developed with the simulated dataset to the real-world dataset, we added only one
set of additional covariates: spectral properties of the catch time series.
Spectral analysis decomposes a time series into the frequency domain and
provides a means of describing the statistical properties of the catch series
that is independent of time series length and absolute magnitude of catch. We fit
spectral models to the scaled catch time series (catch divided by maximum
catch) with the `spec.ar` function in \textsf{R} and recorded representative
short- and long-term spectral densities at frequencies of 0.20 and 0.05, which
correspond to 5- and 20-year cycles.

## Superensemble models

The individual models we seek to combine with superensembles provide time
series of $B/B_\mathrm{MSY}$. Therefore, we can use superensembles to estimate
multiple properties of these status time series. Here, we focus on two
properties: the mean and slope of $B/B_\mathrm{MSY}$ in the last 5 years.
Together, these quantities address the recent state and trajectory of status,
which are both of management and conservation interest [e.g. @mace2008]. To
avoid undue influence of the end points of the time series on the calculated
slope, we measured the slope as the Theil-Sen estimator of median slope
[@theil1950].

Here we compare an ensemble average and three superensembles of varying
complexity: a linear model with two-way interactions, a random forest, and
boosted regression tree. The general form of our models was

$$
\hat{\theta} = 
\argmin_\theta \left( L(b \, | \, \hat{b}_{\mathrm{ensemble}}) \right)
$$
$$
\hat{b}_\mathrm{ensemble} = 
f \left( \hat{b}_\mathrm{CMSY}, \, \hat{b}_\mathrm{COM-SIR}, \,
\hat{b}_\mathrm{SSCOM}, \, \hat{b}_\mathrm{mPRM}, \,
S(0.2), \, S(0.5) \, | \, \theta \right)
$$

\noindent 
where $L$ is a generalized loss function, $f$ is some generalized regression
function, $\hat{b}_\mathrm{CMSY}$ is the mean or slope prediction from the CMSY
model etc. (Table \ref{tab:predictors}), $S(0.2)$ and $S(0.05)$ represent the
spectral density of the scaled catch series at frequencies of 0.20 and 0.05, $b$
is the mean or slope value the model is fitted to, and $\theta$ is the set of
parameters for the regression function. For all superensemble models of mean
$B/B_\mathrm{MSY}$---a ratio bounded at zero---we fit the models in log space
and exponentiated the predictions. For the estimates of $B/B_\mathrm{MSY}$
slope, which are not bounded at zero, we fit superensemble models on the natural
untransformed scale.

We fit the linear model superensemble with second-order interactions. We fit two
machine learning superensemble models based on regression trees. Regression trees
sequentially determine what value of a predictor best splits the response data
into two 'branches' based on a specified loss function [@breiman1984]. In random
forests, a series of regression trees are built on a random subset of the data
and random subset of the covariates of the model [@breiman2001]. In generalized
boosted models (GBMs), each subsequent model is fit to the residuals from the
previous model; data points that are fit poorly in a given model are given more
weight in the next model [@elith2008]. Random forests and GBMs can provide
strong predictive performance and fit highly non-linear relationships
[@elith2008; @hastie2009]. We fit random forest models with the `randomForest`
package [@liaw2002] for \textsf{R} with the default argument values. We fit
boosted regression tree models with the `gbm` package [@ridgeway2015] for
\textsf{R}. Based on cross-validation with the **caret** \textsf{R} package, we
fit GBMs with 2000 trees, an interaction depth of $6$, a learning rate
(shrinkage parameter) of $0.01$, and all other arguments at their default
values.                                                

## Testing model performance

A critical component to any predictive modelling exercise is to evaluate the
performance of a model on new data [@hastie2009]. We used repeated three-fold
cross validation to test predictive performance: we randomly divided the
dataset into three sets, built superensemble models on two-thirds of the data, and
evaluated predictive performance on the remaining third. We repeated this
across each of the three splits and then repeated the whole procedure 50 times
to account for bias that may result from any one set of validation splits.
Since the dynamics of simulated populations differing only in the stochastic
draw of random values are likely similar, we grouped these stocks (10 per
factorial cell) in the cross-validation process into either the training or
testing split. Since mPRM is built on the RAM Legacy Stock Assessment database,
when testing model performance on the stock assessment database, we refit mPRM
on each training split.

Predictive performance can be evaluated with metrics that represent a variety
of modelling goals. For continuous response variables such as the mean and
slope of population status, performance metrics often measure some form of
bias, precision, accuracy (a combination of bias and precision), or the ability
to correctly rank or correlate across populations [e.g. @walther2005]. Here,
we measure proportional error, defined as
$(\hat{b} - b)/b$, where $\hat{b}$ and $b$ represent estimated and 'true'
(or stock-assessed) mean or slope of $B/B_\mathrm{MSY}$. We calculated median proportional
error to measure bias, median absolute proportional error to measure accuracy,
and Spearman's rank-order correlation between predicted and 'true' values to
measure the ability to correctly rank populations. Code to reproduce our
analysis is available at
<https://github.com/datalimited/ensembles>.

# Results

Applied to the simulated dataset of known status, the individual models had
variable success at recovering the mean (status) and slope (trajectory) of
$B/B_\mathrm{MSY}$ in the last five years. All models exhibited a high degree of scatter
around the 1:1 line of perfect status prediction (Fig. \ref{hexagon}). CMSY
exhibited bimodal predictions of status but had the best rank-order correlation
and accuracy scores (Fig. \ref{performance}a). COM-SIR and SSCOM both correctly
identified a number of low status stocks, but frequently predicted a high
status when status was in fact low (Fig. \ref{hexagon}b, c). mPRM had
relatively poor ability to predict status for the simulated dataset (Fig.
\ref{hexagon}d). There was generally little correlation between true and
predicted trajectory for any of the individual models besides SSCOM (Figs
\ref{scatter-sim-slope}a--d).

Ensemble methods, and in particular the machine learning superensemble models (random
forest and GBM), generally improved estimates of status and trajectory over any individual model (Fig.
\ref{hexagon}e--h, Fig. \ref{scatter-sim-slope}e--h). Compared to the
individual models, machine learning superensembles improved accuracy (median
absolute proportional error) by 
<!--TODO: check these percentage values- right?-->
`r mean_sim$mach_vs_ind_mare_fold`%, 
increased rank-order correlation from `r mean_sim$ind_corr_range` 
to `r mean_sim$mach_corr_range`, 
and reduced bias (median proportional error) from 
`r mean_sim$ind_mre_range` to 
`r mean_sim$mach_mre_range` (Fig. \ref{performance}a). 
These superensembles also had better ability to distinguish if
simulated stocks were above or below $B/B_\mathrm{MSY} = 1$ (Fig.
\ref{roc-sim}). Results were similar when predicting trajectory: compared to
individual models, machine learning superensembles improved accuracy by 
`r slope_sim$mach_vs_ind_mare_fold`%, increased rank-order correlation from 
`r slope_sim$ind_corr_range` to 
`r slope_sim$mach_corr_range`, and reduced bias
from `r slope_sim$ind_mre_range` 
to `r slope_sim$mach_mre_range` (Fig. \ref{performance-sim-slope}). The
ensemble models that simply took a mean of the individual models ranked
slightly behind the best individual model for estimating fish stock status (CMSY; Fig.
\ref{performance}a) and had slightly lower correlation but higher accuracy than
the best individual model at predicting the trends of status (SSCOM; Fig.
\ref{performance-sim-slope}).

The superensemble models were able to improve the predictions of status by
exploiting the best properties of individual models, the covariance between
individual models, and interactions with other covariates. For example, SSCOM
had strong predictive ability when it predicted low $B/B_\mathrm{MSY}$ (Fig.
\ref{hexagon}c, Fig. \ref{partial-sim}c) and CMSY predictions were
approximately linearly related to $B/B_\mathrm{MSY}$ within the low and high prediction
clusters (Fig. \ref{partial-sim}). Superensembles also exploited the covariance
between individual model predictions. For instance, both the linear model and
GBM ensemble suggest that if mPRM and SSCOM predict high status, the true
status also tends to be high (Figs \ref{lm-coefs}, \ref{partial-2d-sim}l). The
addition of spectral density covariates helped the superensemble models correctly
predict higher status values (Fig. \ref{hexagon}g, h) by exploiting
interactions between the spectral density predictors and the individual model
predictions (Fig. \ref{partial-2d-sim}). Although the additional covariates
improved ensemble fit, performance of the ensembles was only marginally
degraded by removing these covariates (Fig. \ref{hexagon} vs. Fig.
\ref{hexagon-sim-basic}).

When applied to the stock assessment database, the superensemble models---trained
exclusively on the simulated dataset---generally performed as well or better
than the best individual models. The mean, random forest, and GBM ensembles
outperformed the mPRM method which is trained directly on the RAM Legacy Stock
Assessment database itself (Fig. \ref{performance}b, Fig. \ref{hexagon-ram}).
Compared to the individual models, the machine learning superensembles increased
accuracy by `r mean_ram$mach_vs_ind_mare_fold`%, 
improved correlation from `r mean_ram$ind_corr_range` to 
`r mean_ram$mach_corr_range`, and reduced bias from
`r mean_ram$ind_mre_range` to `r mean_ram$mach_mre_range`.                    

<!--
Some individual models performed nearly as well as the ensembles in one or two performance dimensions: e.g. SSCOM does nearly as well as the ensembles at predicting the slope of $B/B_\mathrm{MSY}$ (Fig. \ref{performance-sim-slope}). But no individual method is consistently nearly as good as the ensembles. SSCOM does more poorly than the ensembles and other individual models in terms of accuracy, rank-order correlation, and bias at estimating the mean $B/B_\mathrm{MSY}$ for both the simulated and stock-assessment datasets (Fig. \ref{performance}).
-->

# Discussion

Ensemble methods provide a useful approach to situations where conservation and
ecological resource management decisions must be made on the basis of multiple,
potentially contrasting estimates of status---a situation common to many
ecological settings beyond fisheries. Compared to individual models of fish
population status, ensemble methods were consistently the best or among the best
across three performance dimensions (accuracy, bias, and rank-order
correlation), two response variables (status and trajectory), two datasets
(simulated and global fisheries), and multiple ensemble methods (from a
simple average to machine learning superensembles). Our results suggest choosing an
superensemble model that allows for non-parametric relationships, such as machine
learning methods. These methods provide added insight into individual model
behaviour and generally performed the best; however, even a simple average of
predictions across multiple models may provide a more useful measure of the
desired parameter than a single model in many cases.

Certain conditions will make some ensemble models more effective than others.
First, ensembles will be most effective when they are comprised of diverse
individual models that choose different structural model forms, explore
contrasting but plausible ranges of parameter values, or make errors in
uncorrelated ways [@ali1996; @dietterich2000; @tebaldi2007]. Such individual
models would be expected to perform well in different conditions and an ensemble
model can exploit the best predictive performance of each. Second, ensemble
models will be most effective when they are not overfit to the training dataset.
Cross-validation testing [@caruana2004; @hastie2009] and methods that are robust
to over-fitting such as random forests [@breiman2001], may help avoid
overfitting ensemble models. We note that our simplest ensemble model, an
average of individual model predictions, performed approximately as well as
complex machine learning models when we trained our superensembles on the
simulation dataset and tested them on a separate 'real' dataset (i.e. the RAM
Legacy Stock Assessment database, Fig. \ref{performance}b). Third, ensemble
models will be most effective when they are trained on data that are
representative of the dataset of interest [@knutti2009; @weigel2010].
Cross-validation within a training dataset will provide an optimistically biased
impression of predictive performance if the training dataset fundamentally
differs from the dataset of interest [@hastie2009].

Multi-model inference in the form of coefficient averaging weighted by
information theoretics such as the Akaike Information Criterion (AIC) is a
common analytical approach in ecology [@burnham2002; @johnson2004;
@grueber2011]. The ensemble methods described in this paper share similarities
with coefficient averaging but differ in other important ways. Ensemble methods
and coefficient averaging share the long-held notion that multiple working
hypotheses can contribute useful information for inference [@chamberlin1890]. A
fundamental difference is that coefficient averaging focuses on averaging
*coefficients* whereas ensembles instead average *predictions*. While the
outcome will be the same in some cases, ensembles provide a more general
purpose tool: they do not require information theoretics and they can combine
different types of models (e.g. parametric and non-parametric models or
frequentist and Bayesian predictions). Furthermore, superensembles extend these
benefits by allowing model predictions be combined in via non-linear functions
that are tuned to known data.

A strength of superensembles is that they can be tailored to predict specific
response variables. For example, we built separate superensemble models of mean
$B/B_\mathrm{MSY}$ and the slope of $B/B_\mathrm{MSY}$. The same set of model
weights or non-linear relationships need not hold across different response
variables. For instance, SSCOM contributed little to the GBM superensemble
estimate of status at higher levels of predicted $B/B_\mathrm{MSY}$, but
contributed strongly to estimates of trajectory (Fig. \ref{partial-sim-slope}).
Formally, fitting superensemble models to specific quantities of interest
provides an additional calibration step [@rykiel1996]. Individual models are
typically calibrated (i.e. parameters are estimated) based on a single response
variable; however, this model might then provide a biased or inaccurate estimate
of other quantities of interest. Ensemble methods provide a final calibration
step to a quantity of interest. This ensemble calibration could further include
a loss function tailored to the goals of the model, say placing greater weight
on accuracy at low status levels than higher values. Conversely, because
superensembles are tailored to a specific response and loss function,
superensembles force a modeller to choose an operational purpose for their model
up front [*sensu* @dickey-collas2014].

As @box1987 noted, all models are wrong, but they may still be useful. The
ensemble methods we investigated attempt to piece together the useful parts of
candidate models to build a model with improved performance. Instead of viewing
the superensemble as a black box, we think considerable mechanistic
understanding can be gained by studying the structure of the superensemble. For
example, when SSCOM estimates very low status this is likely the case,
conversely when COMSIR estimates low status, the truth is likely that the status
is high (Fig. \ref{partial-sim}). These models have two main differences: (1)
the form of effort dynamics and (2) the allowance for both measurement and
process error in SSCOM, whereas the implemented COMSIR admits measurement error
only. Were the methods to differ only in effort dynamics, the results point
towards a more suitable representation of effort dynamics at low biomasses in
SSCOM. We think that such investigation of the parameters of a superensemble may
lead to improvement in the mechanisms imparted to the individual models.

Combining predictions from multiple models via superensemble methods is also
broadly useful in other subfields of ecology and fisheries science. Predictions
about extinction risk are widely used at the national (e.g. the US Endangered
Species Act and the Canadian Species at Risk Act) and international [e.g. the
IUCN Red List, @iucn2015] levels. These risk assessments generally involve
fitting regression models to outcomes for individual species along with
predictors of species risk [e.g. @anderson2011a; @pinsky2011], or population
dynamics models to data for individual species [e.g. @dfo2010]. Both types of
models are prone to error caused by model-misspecification and therefore results
are sensitive to decisions about model structure [@brooks2015]. Although there
are options to account for potential model-misspecification in determination of
species risk [e.g. coefficient averaging, @burnham2002; generalized modeling,
@yeakel2011; or semi-parametric methods, @thorson2014], ensemble methods are
a relatively simple way to combine predictions in a transparent manner. Beyond
estimates of status and trajectory, ensemble methods could be used to increase
the robustness of spatial predictions when designing networks of protected areas
[@rassweiler2014], or to forecast potential spatial shifts in species
distribution given climate impacts [@harsch2014].
	
<!--

and different models may result in different scalings of population status
[Deroba JJ, Butterworth DS, Methot RD Jr., De Oliveira JAA, Fernandez C, Nielsen
A, Cadrin SX, Dickey-Collas M, Legault CM, Ianelli J, et al. 2015. Simulation
testing the robustness of stock assessment models to error: some results from
the ICES Strategic Initiative on Stock Assessment Methods. ICES Journal of
Marine Science 72: 19–30 doi:10.1093/icesjms/fst237]
CM: It would be good to keep this in mind from a management perspective as
depending on the assumptions in the assessment, biomass could be high and
fishing mortality low to predict the same catches as biomass low and fishing
mortality high. Combining for a single scaled variable B/Bmsy removes some of
the scaling but when there is a strong covariation in B and F this would need to
be (and likely can be) accounted for in a bivariate ensemble. Not our focus here
but could enter the Discussion.


Biological ensemble modeling to evaluate potential futures of living marine
resources
Anna Gårdmark1,8, Martin Lindegren2, Stefan Neuenfeldt2, Thorsten Blenckner3,
Outi Heikinheimo4, Bärbel Müller-Karulis3,5, Susa Niiranen3, Maciej T. Tomczak3,
Eero Aro4, Anders Wikström6, and Christian Möllmann7
Read More: http://www.esajournals.org/doi/abs/10.1890/12-0267.1

* Other possible topics:
    * how much better are the ensembles really? translate the results into
      some concrete examples in an absolute not relative sense
    * other types of ensembles: assigning weights, incorporating uncertainty
      around estimates, GAMs, ...
    * when wouldn't you want to use ensembles? perhaps if models represent 
      dichotomous assumptions where either one or the other is right and imply
      different management actions? ...
    * your ideas here...

Why do ensembles work?

- 'representational' no one model can contain all 'hypothetical functional
  forms'... many models can cover many hypotheses @dietterich2000
- error cancellation --- especially if multiple models make errors in
  uncorrelated ways [@ali1996]
- they provide an additional step of 'calibration' [@rykiel1996] where the
  predictions are calibrated to data of known or assumed truth about a
  particular aspect of interest in the data (e.g. mean status, slope of status)
  whereas individual model may be calibrated (parameters estimated) with a
  slightly different response in mind (e.g. maximizing likelihood with respect
  to observed and predicted catches)
- we usually do not know which individual model is 'best' [@hagedorn2005];
  ensembles have the property of rarely being far from the best
-->

# Acknowledgements

<!--Place the acknowledgment paragraph on the cover page of your manuscript.
(Reviewers are not provided with a cover page.) Do not spell out first (given)
names. Provide the first initial of the first name, even if the initial starts a
sentence. Do not use titles (e.g., Dr. or Professor). Refer to authors of the
manuscript by their initials only (e.g., “S.T.W. was supported by a grant from
the Torry Foundation.”).-->
We thank members of Phase I of the working group 'INSERT NAME HERE' who
contributed to developing the data-limited methods and simulations used in our
analysis.
We thank J.A. Hutchings for helpful discussions on ensemble methods for
assessing population status.
We thank the Gordon and Betty Moore Foundation for funding of the working group
'INSERT NAME HERE'.

<!--Funding...-->

<!--# Citation notes-->

<!--
- @breiner2015 ensemble models of species distribution models for rare species

- @jones2015 ensemble models of species distribution models - globally for
  marine biodiversity

- [@greene2006] example similar to ours but with climate models (Bayesian
  multilevel ensemble)

- [@kell2007] FLR

- multiple learners book [@alpaydin2010]

- [@caruana2004] nice paper on ensemble models; prob of overfitting increases
  with more models, bagging important

- [@tebaldi2007] key paper: review of 'multi-model ensembles for climate projections'

- famous ensemble is DEMETER: Development of a European Multi-model Ensemble
  System for Seasonal to Interannual Prediction @hagedorn2005 is a good
  reference

- Examples of where ensemble models are shown to be better than any one:
  @thomson2006 (public health - malaria), @cantelaube2005 (agriculture - crop
  yield) (citations taken from @tebaldi2007)

- simple averages are used very commonly in climate science - e.g. IPCC 2001

- @tebaldi2007: weighting obviously makes sense, but how do we define a
  performance metric that is based on past observations that is relevant to the
  future? 

- @tebaldi2007: model independence important

- for climate, weighted averages perform better than simple averages [@min2006],
  but are those same weights applicable to the future (or in our case other
  fisheries)?

Why do ensemble methods work?

- error cancellation is one but not the only reason for superiority of ensemble
  models [@hagedorn2005]

- ensemble model may be only marginally better than the best single model in
  any given case, but we don't usually know which is the best single model
  [@hagedorn2005]

- ensemble models useful for regional climate models too; as an example,
  @pierce2009 use 42 metrics to characterize model performance in regional
  climate model ensembles; found that ensemble models were superior to any one
  model --- especially when considering multiple metrics

- @knutti2009: key review paper on motivation and challenges of combining
  climate projection models; performance on testing / current data may only
  weakly relate to future / other datasets

- @murphy2004: Nature paper on ensembles of climate model simulations; weighted
  average better performance than unweighted average

- @dietterich2000: highly cited book chapter "Ensemble Methods in Machine
  Learning"; ensembles are often much more accurate than the "individual
  classifiers that make them up"; but components must be diverse and accurate

- @dietterich2000: a main justification for ensembles: they are
  representational --- no one model usually can contain all hypothetical
  functional forms, but many separate models can cover more hypotheses

- strong paper showing that ensemble models are most accurate when the various
  individual models make errors in uncorrelated ways [@ali1996]

- without substantial training-testing data, appropriate model weights can be
  very hard to deduce and can cause more harm than good (compared to equal
  weighting) [@weigel2010] (they give the example of seasonal forecasting where
  a ton of hind cast testing can be done, vs. long-term climate) (asymmetrical
  loss function)

- [@weigel2010] optimal weights are always as good or better than equal
  weights, but if you get the weights wrong, you can be better off just using
  equal weights; but averaging was almost always better than any one model

- @rykiel1996 "Testing ecological models: the meaning of validation" (see for
  performance criteria, theory on model testing and assessment)

model averaging AIC:
f Chamberlin 1890).  multiple woring hypotheses
  (e.g., Johnson and Omland 2004, Hobbs and Hilborn 2006, Burnham et al. 2011,
  52 Grueber et al. 2011). 
-->

