<!-- TODO
- [ ] add discussion of uncertainty (CIs) and ML (and incorporating uncertainty into pred.)
- [x] add back in the roc plot and sentence in the main text
- [x] rerun the main simulation analysis because the equations of change for the slope
- [x] manually adjusted the labels in figure 4
- [x] update the magic number get inserted to use the equation that only has the basic model
- [x] check figure order
- [ ] check all remaining in-text todo items
-->

```{r, echo=FALSE}
# Values to use in the paper:
load("values.rda") # generated by `../analysis/9-output-values.R`
# if (mean_sim_basic$mach_mare_range == "0.32--0.32") mean_sim_basic$mach_mare_range <- 0.32
# Includes: mean_sim, mean_ram, slope_sim, auc_sim, ram_stocks_n

# mean_sim, mean_ram, and slope_sim have the following list elements:
# > names(mean_sim)
# "mach_vs_ind_mare_fold" "ind_corr_range" "mach_corr_range" "ind_mare_range"
# "mach_mare_range" "ensemble_mre_range" "ind_mre_range"

# auc_sim has the following list elements:
# > names(auc_sim)
# "mach_range" "ind_range"
```

<!--
 The full title of the paper, with two alternatives. Keep titles short and simple.
• The full names of all authors.
• The name(s) and address(es) of the institution(s) at which the work was
carried out. If the present address of any author is different from the above,
give it as a footnote.
• The name, address, telephone, fax and e-mail contacts of the author to whom
all correspondence and proofs should be sent.
• A suggested running title of not more than 40 characters, including spaces.
-->

\begin{Large}
\noindent
Improving estimates of population status and trend with superensemble models
\end{Large}

\noindent
Alternative 1: Combining stock-assessment output with ensemble modelling

\noindent
Alternative 2: Ensembles of data-limited stock assessments improve accuracy
and reduce bias of $B/B_\mathrm{MSY}$ estimates

\bigskip

\noindent
Sean C. Anderson^1\*^,
Andrew B. Cooper^1^,
Olaf P. Jensen^2^,
C\'{o}il\'{i}n Minto^3^,
James T. Thorson^4^,
Jessica C. Walsh^1^,
Jamie Afflerbach^5^,
Mark Dickey-Collas^6,7^,
Kristin M. Kleisner^8^,
Catherine Longo^9^,
Giacomo Chato Osio^10^,
Daniel Ovando^11^,
Iago Mosqueira^10^,
Andrew A. Rosenberg^12^,
Elizabeth R. Selig^13^

\bigskip

\noindent
^1^School of Resource and Environmental Management,
Simon Fraser University, Burnaby, BC, V5A 1S6, Canada

\noindent ^2^Institute of Marine & Coastal Sciences, Rutgers University,
71 Dudley Road, New Brunswick, NJ, 08901-8525, USA

\noindent ^3^Marine and Freshwater Research Centre, Galway-Mayo Institute of
Technology, Galway, 00000, Ireland

\noindent ^4^Fisheries Resource Analysis and Monitoring Division, Northwest
Fisheries Science Center, National Marine Fisheries Service, National
Oceanographic and Atmospheric Administration, 2725 Montlake Boulevard E.,
Seattle, WA, 98112, USA

\noindent ^5^National Center for Ecological Analysis and Synthesis, University
of California Santa Barbara, 735 State Street, Suite 300, Santa Barbara, CA,
93103, USA

\noindent ^6^International Council for the Exploration of the Sea, H.C.
Andersens Boulevard 44-46, DK 1553, Copenhagen, Denmark

\noindent ^7^DTU Aqua National Institute of Aquatic Resources, Technical
University of Denmark (DTU), Jægersborg Alle 1, 2920 Charlottenlund, Denmark

\noindent ^8^Ecosystem Assessment Program, Northeast Fisheries Science Center,
National Marine Fisheries Service, National Oceanographic and Atmospheric
Administration,
166 Water St., Woods Hole, MA, 02543, USA

\noindent ^9^Marine Stewarship Council, Marine House, 1 Snow Hill, London, EC1A
2DH, United Kingdom

\noindent ^10^European Commission, DG Joint Research Centre, Directorate D --
Sustainable Resources, Unit D.02 Water and Marine Resources, Via E. Fermi 2749,
21027 Ispra VA, Italy

\noindent ^11^Bren School of Environmental Science and Management, University of
California Santa Barbara, Santa Barbara, CA, 93106-5131, USA

\noindent ^12^Union of Concerned Scientists, Cambridge, MA, USA

\noindent ^13^Conservation International, Arlington, VA, USA

\noindent ^\*^Corresponding author, present address: School of Aquatic and
Fishery Sciences, Box 355020, University of Washington, Seattle, WA  98195, USA
E-mail: sandrsn@uw.edu

\bigskip

\noindent Running title: Superensembles of population status



\clearpage
<!--up to 250 words-->

\bigskip

\noindent \textbf{Abstract:} Fishery managers must often reconcile conflicting
estimates of population status and trend. Superensemble models, commonly used in
climate and weather forecasting, may provide an effective solution. This
approach uses predictions from multiple models as covariates in an additional
"superensemble" model fitted to known data. We evaluated the potential for
ensemble averages and superensemble models ("ensemble methods") to improve
estimates of population status and trend for fisheries. We fit four widely
applicable data-limited models that estimate stock biomass relative to the
equilibrium biomass at maximum sustainable yield ($B/B_\mathrm{MSY}$). We
combined these estimates of recent fishery status and trends in $B/B_\mathrm{MSY}$
with four ensemble methods: an ensemble average and three superensembles (a
linear model, random forest, and boosted regression tree).  We trained our
superensembles on 5760 simulated stocks and tested them with
cross-validation and against a global database of `r ram_stocks_n` stock
assessments. Ensemble methods substantially improved estimates of population
status and trend. Random forest and boosted regression trees performed the best
at estimating population status: inaccuracy (median absolute proportional error) decreased from
`r mean_sim_basic$ind_mare_range` to `r mean_sim_basic$mach_mare_range`,
rank-order correlation between predicted and
true status improved from `r mean_sim_basic$ind_corr_range` to
`r mean_sim_basic$mach_corr_range`, and bias (median proportional error) declined from
`r mean_sim_basic$ind_mre_range` to `r mean_sim_basic$mach_mre_range`.
We found similar improvements when predicting trend and when applying the
simulation-trained superensembles to catch data for global fish stocks.
Superensembles can optimally leverage multiple model predictions;
however, they must be tested, formed from a diverse set of accurate models, and
built on a dataset representative of the populations to which they are applied.

\bigskip

\noindent Keywords: CMSY, data-limited fisheries, ensemble methods, multi-model
averaging, population dynamics, sustainable resource management

\tableofcontents


# Introduction

Status and trend are two of the most fundamental values to quantify in the
management of ecological populations [e.g., @hutchings2010; @iucn2015]. However,
managers are often faced with reconciling multiple uncertain and potentially
conflicting estimates of status and trend [e.g., @brodziak2010; @branch2011;
@deroba2015]. For example, one model may suggest a population is at risk and
declining in abundance while others may suggest it is not at risk and stable.

One solution is to take the average or weighted average of several model
predictions, i.e., an ensemble. Such ensembles are typically more accurate and
less biased than individual model estimates and can incorporate various types of
uncertainty, such as uncertainty in model structure, initial conditions, and
parameter estimation [@dietterich2000; @araujo2007]. Ensembles are superior to
individual models in at least three ways: (1) statistically by averaging across
models and therefore being less likely to pick the "wrong" model, (2)
computationally by reducing the risk of getting stuck in a local optima, and (3)
representationally by expanding the range of hypotheses explored
[@dietterich2000]. This approach forms the basis of many machine learning
methods [e.g., @dietterich2000], has helped reconcile climate forecasts from
dozens of models [e.g., @murphy2004; @tebaldi2007; @ipcc2013], and even improved
early warning signs of malaria outbreaks [@thomson2006]. In ecology, ensemble
methods are sometimes used to improve species distribution modelling [e.g.,
@araujo2007; @breiner2015] and indeed have been used to combine estimates of
population status and trend [e.g., @brodziak2010].

Whereas averages or weighted averages of model estimates may improve
predictions compared to a single model, they may not optimally leverage
available data. The best prediction does not necessarily lie in the middle of
multiple model predictions, some models may perform better than others in
certain conditions, and the covariance between models may contain information
that can improve predictive accuracy. For example, one model might perform
well at estimating high levels of abundance but be biased at low levels of
abundance, while another model might have the opposite properties. An optimal
combination of these models is not simply an average of the two.

We can exploit these characteristics by using the predictions from a group of
models as inputs into a separate statistical model. This technique, sometimes
called superensemble modelling [@krishnamurti1999], is common in climate and
weather forecasting [e.g., @yun2005; @mote2015]. The superensemble is fit to a
training dataset where outcomes are well known and then used to predict on a
dataset of interest. For example, @krishnamurti1999
combined predictions of wind and precipitation in Asian monsoons via a
superensemble regression fit to observed data. Their superensemble was
considerably more accurate than any individual prediction or an average
of the predictions.

In fisheries science, the commonly used operational models for determining
status and trend of exploited fish populations are stock assessments, i.e.,
population models coupled to an observation model that incorporate all
appropriate data (e.g., catches, size and age distributions, surveys, and
tagging information) to quantify values such as the biomass of a stock that
can produce maximum sustainable yield ($B_\mathrm{MSY}$) [@hilborn1992].
However, the broad range of data required to conduct these stock assessments
are not available for the majority of fish populations, including those of
conservation concern and of economic interest to fisheries [@fao2014].
Therefore, a number of models have been proposed to assess $B/B_\mathrm{MSY}$
based on the limited data available for the majority of fish stocks: (1) a time
series of the total weight of catch and (2) a basic understanding of
population productivity [e.g., @vasconcellos2005; @martell2013]. Recently,
@rosenberg2014 investigated the performance of four data-limited models through
a large-scale simulation experiment. Three of these models were based on
Schaefer (logistic) biomass dynamics and one was an empirical model fitted to
more data-rich stock-assessment output. The four models frequently disagreed
about population status (e.g., Fig. \ref{motivate}), no one model had strong
performance across all fish stocks, and some models performed better than
others depending on circumstances.

Here, we estimate population status and trend of exploited fish populations
using ensembles and superensembles (collectively "ensemble methods") of these
four data-limited models. We apply four ensemble-method approaches of varying
complexity to both simulated and real-world fish stocks and compare their
predictive performance against each other and the individual models.

# Methods

To test the ability of superensembles to improve estimates of status and trend
in data-limited fish stocks, we first fit four individual assessment models to a
large simulated dataset of fish stocks. We then built and tested the performance
of superensembles using cross-validation of the simulated dataset. Finally, we
tested superensembles built with the entire simulated dataset against a database
of global fish stocks. We describe these steps in detail below and illustrate
the general approach both with illustrations and pseudocode in Fig. 2.

## Individual models of population status

We fit four individual data-limited models that use catch data and basic
life-history parameters to estimate $B/B_\mathrm{MSY}$. We chose these models
because they can be fit to the vast majority of fisheries around the
world, are established in the literature, and have been extensively simulation
tested [@rosenberg2014].

Three of the models are mechanistic and based generally on Schaefer biomass
dynamics [@schaefer1954] of the form

$$\hat{B}_{t+1} = B_t + r B_t \left(1 - B_t / B_0 \right) - C_t,$$

\noindent where $\hat{B}_{t+1}$ represents predicted biomass at time $t$ plus
one year, $B_t$ represents biomass at time $t$, $r$ represents intrinsic
population growth rate, $B_0$ represents unfished biomass or carrying capacity
$K$, and $C$ represents catch. The fourth model is an empirically derived model
based on the RAM Legacy Stock Assessment Database [@ricard2012]. @rosenberg2014
provide a full background on these four methods \R{C} 
(<http://www.fao.org/docrep/019/i3491e/i3491e00.htm>, last accessed 2016-11-08) 
and code to fit all the models
is available in an accompanying package `datalimited` for the statistical
software R [@r2015] <https://github.com/datalimited/datalimited> 
(last accessed 2016-11-08). In summary:

* *CMSY* (catch-MSY) implements a stock-reduction analysis with Schaefer biomass
  dynamics [@martell2013]. It requires a prior distributions on $r$ and $K$ as
  well as priors on the relative proportion of biomass at the beginning and end
  of the time series compared to unfished biomass (depletion). The version of
  the model used in @rosenberg2014 was modified from @martell2013 to generate
  biomass trends from all viable $r$-$K$ pairs and produce an estimate of
  $B/B_\mathrm{MSY}$ from the median trend.

* *COM-SIR* (catch-only-model with sampling-importance-resampling) is
  a coupled harvest-dynamics model [@vasconcellos2005]. Biomass is assumed to
  follow a Schaefer model and harvest dynamics are assumed to follow
  a logistic model. The model is fit with a sampling-importance-resampling
  algorithm [@rosenberg2014].

* *SSCOM* (state-space catch-only model) is a hierarchical model that, similar
  to COM-SIR, is based on a coupled harvest-dynamics model [@thorson2013].
  SSCOM estimates unobserved dynamics in both fishing effort and the fished
  population based on a catch time series and priors on $r$, the maximum rate
  of increase of fishing effort, and the magnitude of various forms of
  stochasticity. The model is fit in a Bayesian state-space framework to
  integrate across three forms of stochasticity: variation in effort,
  population dynamics, and fishing efficiency [@thorson2013].

* *mPRM* (modified panel regression model) is a modified version of the
  panel-regression model from @costello2012. Unlike the other models, mPRM is
  empirical and not mechanistic --- it uses the RAM Legacy Stock Assessment
  database to fit a regression model to a series of characteristics of the
  catch time series and stock with stock-assessed $B/B_\mathrm{MSY}$ as the
  response. The model used in this paper is modified from the original --- it
  condenses the life-history categories into three categories to match the
  simulated dataset, removes the maximum catch predictor since the absolute
  catch in the simulated dataset is arbitrary, and does not implement the bias
  correction needed in @costello2012 since we do not derive estimates of
  median status across multiple stocks.

## Simulated dataset to build the superensemble

We first developed and tested ensemble methods on a fully factorial simulated
dataset of fisheries with known status [@rosenberg2014]. 
Briefly, these simulations were implemented 
with the FLR packages [@kell2007] 
for the statistical software R, and, in particular, the FLBRP package.
The framework takes a series of 
life-history parameters and fishery characteristics
to generate population projections
and resulting catch timeseries.
Life-history values (e.g. mean asymptotic length)
for three fish life histories (small pelagic,
demersal, and large pelagic) 
were translated into a complete set of parameters for
a von Bertalanffy growth model,
a maturity ogive,
natural mortality, 
a selectivity function,
and a Beverton-Holt stock-recruitment function\R{A4}
using the life-history relationships derived in @gislason2008.

Fishing scenarios included three levels of initial biomass depletion
compared to carrying capacity: biomass at 100%, 70%, and 40% of carrying
capacity; and four exploitation patterns: (1) a constant exploitation rate,
(2) an exploitation rate coupled with biomass to mimic an open-access
single-species fishery, (3) a scenario where exploitation rate increased
continuously, and (4) a "roller-coaster" scenario where the exploitation
rate increased and then decreased. Process noise (recruitment variability;
i.e., unexplained variability in population dynamics) was introduced to the
models at two magnitudes in log space, $N(0, 0.2^2)$ and $N(0, 0.6^2)$,
and was either uncorrelated through time or had first-order autoregressive
correlation of $0.6$. The simulation also included a scenario with $N(0,
0.2^2)$ measurement error around log catch  and one scenario without
measurement error. @rosenberg2014 ran ten iterations for each combination
of factors adding stochastic draws of recruitment and catch-recording
variability each time to generate a total of 5760 stocks. 
Code to generate the simulations is available at
<https://github.com/datalimited/stocksims> (last accessed 2016-11-08).

## Building the superensemble models

The individual models we seek to combine with superensembles provide time series
of stock status ($B/B_\mathrm{MSY}$). Therefore, we can use superensembles to
estimate any property of these time series. Here, we focus on two properties:
the mean and slope of $B/B_\mathrm{MSY}$ in the last five years. Together, these
quantities address the recent state and trend of stock status, which are both of
management and conservation interest [e.g., @hutchings2010; @iucn2015]. To avoid
undue influence of the time series end points on the calculated slope, we
measured the slope as the Theil-Sen estimator of median slope [@theil1950].

We used the mean or slope of $B/B_\mathrm{MSY}$ as the response variable and the
predictions from the individual models as predictors in our superensemble models
(Fig. \ref{didactic}a). When modelling mean $B/B_\mathrm{MSY}$ --- a ratio
bounded at zero --- we fit the superensemble models in log space and
exponentiated the predictions. For the estimates of $B/B_\mathrm{MSY}$ slope,
which are not bounded at zero, we fit superensemble models on the natural
untransformed scale.

We compared an ensemble average and three superensembles of varying complexity:
a linear model with two-way interactions, a random forest, and a boosted
regression tree. We describe these models as estimating $\hat{\theta}$, which
represents either the ensemble estimated $\log B/B_\mathrm{MSY}$ or slope of
$B/B_\mathrm{MSY}$. The individual model estimates of $\log B/B_\mathrm{MSY}$ or
slope of $B/B_\mathrm{MSY}$ are represented as $\hat{b}$ for models $i$ $1$
through $4$ (CMSY, COM-SIR, SSCOM, mPRM). The ensemble average for each fishery
$i$ was calculated as:

\begin{equation}
\hat{\theta_i} = \left( \hat{b}_{i,1} + \hat{b}_{i,2} + \hat{b}_{i,3} + \hat{b}_{i,4} \right) / 4, \quad
\mathrm{for}\ i  =  1,...,n.
\end{equation}

\noindent We fit the linear model superensemble with all second-order
interactions:

\begin{equation}
\hat{\theta_i} = \beta_0 + \beta_1 \hat{b}_{i,1} + ... +
\beta_{1,2} \hat{b}_{i,1} \hat{b}_{i,2} + ... +
\epsilon_i,
\quad
\epsilon \sim \mathrm{Normal}(0, \sigma^2),
\quad \mathrm{for}\ i  =  1,...,n.
\end{equation}

\noindent \R{A5}For this illustrative example 
we chose this level of model complexity *a priori*
but a modeller could apply model selection via information-theoretic
or cross-validation approaches. 

Our two machine learning superensemble models, a random forest and a
generalized boosted model (GBM), were based on regression trees. Regression
trees sequentially determine what value of a predictor best splits the
response data into two branches based on a loss function [@breiman1984]. In
random forests, a series of regression trees are built on a random subset of
the data and random subset of the covariates of the model [@breiman2001]. In
GBMs, each subsequent model is fit to the residuals from the previous model;
data points that are fit poorly in a given model are given more weight in the
next model [@elith2008]. Random forests and GBMs can provide strong predictive
performance and fit highly non-linear relationships [@elith2008; @hastie2009].
We fit random forest models with the `randomForest` package [@liaw2002] for R
with the default argument values. We fit boosted regression tree models with
the `gbm` package [@ridgeway2015] for R. We fit GBMs with 2000 trees, an
interaction depth of $6$, a learning rate (shrinkage parameter) of $0.01$, and
all other arguments at their default values.

## Additional covariates

Superensemble models allow us to incorporate additional covariates and
potentially leverage interactions between these covariates and individual model
predictions. Additional covariates could be, for example, life-history
characteristics, information on exploitation patterns, or statistical
properties of the data. We tested the performance benefits of including one set
of additional covariates: spectral properties of the catch time series.
Spectral analysis decomposes a time series into the frequency domain and
provides a means of describing the cyclical shape of the catch series that is
independent of time series length (except in affecting precision) and
independent of absolute magnitude of catch. We fit spectral models to the
scaled catch time series (catch divided by maximum catch) with the `spec.ar`
function in R and recorded representative short- and long-term spectral
densities at frequencies of 0.20 and 0.05, which correspond to 5- and 20-year
cycles. For the linear model superensemble, we incorporated the two spectral
covariates ($S1$, $S2$) along with all second-order interactions as:\R{D}

\begin{equation}
\hat{\theta_i} = \beta_0 + \beta_1 \hat{b}_{i,1} + ... +
\beta_{1,2} \hat{b}_{i,1} \hat{b}_{i,2} + ... +
\beta_{S1} S1_i + 
\beta_{S2} S2_i + 
\beta_{S1,S2} S1_i S2_i + 
\beta_{1,S2} \hat{b}_{i,1} S2_i + ... +
\epsilon_i,
\end{equation}

\noindent
with $\epsilon \sim \mathrm{Normal}(0, \sigma^2)$ and for
simulated fisheries $i$ 1 through $n$.
We include the results of adding these additional covariates in the
supplementary materials.

## Applying the superensemble models and testing performance

Once the superensemble models are built and trained using the simulated stocks
(or any dataset with "known" status), we can use the superensembles to estimate
the status of new stocks (Fig. \ref{didactic}b). To do this we applied the
individual models to our stocks of interest (i.e., CMSY, COM-SIR, SSCOM, mPRM)
and then used these individual model estimates of status or trend as data in our
already built superensemble models. In this paper we applied the superensemble
models to subsets of the simulated data as a cross-validation test to test
predictive performance and to the RAM Legacy Stock Assessment database to test
predictive performance on real stocks.

We used repeated three-fold cross validation: we randomly divided the dataset
into three sets, built superensemble models on two-thirds of the data, and
evaluated predictive performance on the remaining third. We repeated this across
each of the three splits and then repeated the whole procedure 50 times to
account for bias that may result from any one set of validation splits. In the
simulated dataset, there were 10 replicates of each unique combination of
simulation parameters that differed only in stochastic variability. Since the
dynamics of these populations were often similar, we grouped these stocks in the
cross-validation process into either the training or testing split.

We also tested our ensemble methods on the RAM Legacy Stock Assessment Database
--- a compilation of stock-assessment output from hundreds of exploited marine
populations around the world. Our analysis of the stock-assessment database was
based on version 2.5. After removing stocks for which at least one of the
individual models did not converge (121), this database
included `r ram_stocks_n` stocks. We removed these stocks for all methods ---
both for the individual and superensemble models. An alternative would be to
fit separate superensemble models to subsets of the individual models that did converge,
but for simplicity we only used superensemble models fitted to all four
individual models.\R{A6}

In the case of the RAM Legacy Stock Assessment Database, we used superensembles
trained on the entire simulation dataset. However, since mPRM is built on the
same stock-assessment database, we applied three-fold cross-validation to the
data underlying the mPRM model so that the dataset with which mPRM was trained
(for the individual model and superensemble) was separate from the dataset with
which it was tested. This meant that, for each iteration of cross validation, 
we split the RAM database into three, fit the mPRM model to two-thirds of
the RAM database, fit a superensemble with this version of mPRM, and then 
tested the performance of the superensemble on the third of the RAM database
we had withheld.\R{A7}

Predictive performance can be evaluated with metrics that represent a variety
of modelling goals. For continuous response variables such as the mean and
slope of population status, performance metrics often measure some form of
bias, precision, accuracy (a combination of bias and precision), or the ability
to correctly rank or correlate across populations [e.g., @walther2005]. Here, we
measure proportional error, defined as $(\hat{\theta} - \theta)/\abs{\theta}$,
where $\hat{\theta}$ and $\theta$ represent estimated and "true" (or
stock-assessed) mean or slope of $B/B_\mathrm{MSY}$. We calculated median
proportional error to measure bias, median absolute proportional error to
measure accuracy, and Spearman's rank-order correlation between predicted and
"true" values to measure the ability to correctly rank populations. When
testing with the RAM Legacy Stock Assessment database, we treated the
estimates from these data-rich stock assessments as known without error. Thus,
any error in the stock-assessment estimates of the mean or slope of
$B/B_\mathrm{MSY}$ also contributes to our estimates of prediction error for
each of the four data-limited models and the ensembles. Code to reproduce our
analysis is available at <https://github.com/datalimited/ensembles> (last accessed 2016-11-08).

# Results

Applied to the simulated dataset of known stock status, the individual models
had variable success at estimating the mean (status) and slope (trend) of
$B/B_\mathrm{MSY}$ in the last five years. All models exhibited a high degree of
scatter around the one-to-one line of perfect status prediction (Fig.
\ref{hexagon}). In contrast to the known unimodal distribution of status, CMSY
exhibited bimodal predictions (Fig. \ref{hexagon}a), but had the best rank-order
correlation and accuracy scores (Fig. \ref{performance}a). COM-SIR and SSCOM
both correctly identified a number of stocks with low status, but frequently
predicted a high status when status was in fact low (Fig. \ref{hexagon}b, c).
mPRM had relatively poor ability to predict status for the simulated dataset
(Fig. \ref{hexagon}d). There was generally little correlation between true and
predicted recent trend in status for any of the individual models (rank-order
correlation = `r range_slope_corr_non_sscom`) with the exception of SSCOM
(correlation = `r slope_corr_sscom`; Figs \ref{scatter-sim-slope}a--d).

Ensemble methods, and in particular the machine learning superensemble models\R{A8}
(random forest and GBM), generally improved estimates of status and trend over
any individual model (Fig. \ref{hexagon}e--h, Fig. \ref{scatter-sim-slope}e--h).
Compared to the individual models, machine learning superensembles decreased
inaccuracy (median absolute proportional error) from
`r mean_sim_basic$ind_mare_range` to `r mean_sim_basic$mach_mare_range`,
increased rank-order correlation from
`r mean_sim_basic$ind_corr_range` to `r mean_sim_basic$mach_corr_range`, and reduced bias
(median proportional error) from `r mean_sim_basic$ind_mre_range` to
`r mean_sim_basic$mach_mre_range` (Fig. \ref{performance}a).
These superensembles also generally had better ability to distinguish if
simulated stocks were above or below $B/B_\mathrm{MSY} = 0.5$ (Fig. \ref{roc-sim}).
Results were similar when predicting trend: compared to individual models,
machine learning superensembles decreased inaccuracy from
`r slope_sim$ind_mare_range` to `r slope_sim$mach_mare_range`,
increased rank-order correlation from
`r slope_sim$ind_corr_range` to `r slope_sim$mach_corr_range`, and reduced
bias from `r slope_sim$ind_mre_range` to `r slope_sim$mach_mre_range` (Fig.
\ref{performance-sim-slope}).
The ensemble models that simply took a mean of the individual models ranked
slightly behind the best individual model for estimating fish stock status
(CMSY; Fig. \ref{performance}a) and had slightly lower correlation but higher
accuracy than the best individual model at predicting the trends of status
(SSCOM; Fig. \ref{performance-sim-slope}).

The superensemble models were able to improve the predictive performance by
harnessing the best properties of individual models, the covariance between
individual models, and interactions with other covariates. For example, SSCOM
had strong predictive ability when it predicted low $B/B_\mathrm{MSY}$ (Fig.
\ref{hexagon}c, Fig. \ref{partial-sim}c) and CMSY predictions were approximately
linearly related to $B/B_\mathrm{MSY}$ within the low and high clusters of
predictions (Fig. \ref{partial-sim}). SSCOM contributed most strongly on its own
to determining trend (Fig. \ref{partial-sim-slope}). Superensembles also
exploited the covariance between individual model predictions. For instance,
both the linear model and GBM ensemble suggest that if mPRM and SSCOM predict
high status, the true status also tends to be high (Figs \ref{lm-coefs},
\ref{partial-2d-sim}f). \R{A9}The addition of spectral density covariates helped the
superensemble models correctly predict higher status values (Fig.
\ref{hexagon-sim-covariates}g, h). The performance of the ensembles was only
marginally improved by including these covariates (Fig.
\ref{performance-with-covariates} vs. Fig. \ref{performance}).

When applied to the stock-assessment database, the superensemble models ---
trained exclusively on the simulated dataset --- generally performed as well or
better than the best individual models. The mean, random forest, and GBM
ensembles outperformed the mPRM method which is trained directly on the RAM
Legacy Stock Assessment database itself (Fig. \ref{performance}b, Fig.
\ref{hexagon-ram}). Compared to the individual models, the machine learning
superensembles increased accuracy by
`r mean_ram$mach_vs_ind_mare_fold`%, improved correlation from
`r mean_ram$ind_corr_range` to `r mean_ram$mach_corr_range`, and reduced bias
from `r mean_ram$ind_mre_range` to `r mean_ram$mach_mre_range`.

# Discussion

Ensemble methods provide a useful approach to situations where environmental
resource management decisions must be made on the basis of multiple,
potentially contrasting estimates of status. Compared to individual models of
fish population status, ensemble methods were consistently the best or among
the best across three performance dimensions (accuracy, bias, and rank-order
correlation), two response variables (status and trend), two datasets
(simulated and global fisheries), and multiple ensemble methods (from a simple
average to machine learning superensembles). Our results suggest choosing a
superensemble model that allows for non-linear relationships, such as
machine learning methods; these models provided added insight into individual
model behaviour and generally performed the best.

Certain conditions will make some ensemble models more effective than others.
First, ensembles will be most effective when they are comprised of diverse
individual models that choose different structural model forms, explore
contrasting but plausible ranges of parameter values, and make uncorrelated
errors [@ali1996; @dietterich2000; @tebaldi2007]. We would expect such models
to perform well in different conditions and an ensemble model can
exploit the best predictive performance of each. Second,
ensemble models will be most effective when they are not overfit to the
training dataset. Cross-validation testing [@caruana2004; @hastie2009] and
methods that are robust to overfitting such as random forests [@breiman2001],
may help avoid overfitting ensemble models. We note that our simplest ensemble
model, an average of individual model predictions, performed approximately as
well as complex machine learning models when we trained our superensembles on
the simulation dataset and tested them on a separate "real" dataset (i.e., the
RAM Legacy Stock Assessment database, Fig. \ref{performance}b). Third, ensemble
models will be most effective when they are trained on data that are
representative of the dataset of interest [@knutti2009; @weigel2010].
Cross-validation within a training dataset will provide an optimistically
biased impression of predictive performance if the training dataset
fundamentally differs from the dataset of interest [@hastie2009].

<!-- It is often important to provide measures of uncertainty with quantitative -->
<!-- estimates. --> 
\R{B}
We illustrated that superensembles can improve point
estimates of population status and trends in status; however, there is no reason why
superensembles cannot also be used to
provide measures of uncertainty around those point estimates. 
The same approaches to deriving measures of uncertainty from any regression model are available to a superensemble. 
For example, likelihood profile confidence intervals or Bayesian credible intervals 
are available for superensembles fit via 
maximum likelihood or Bayesian procedures, respectively.
Measures of predictive uncertainty can be generated for machine learning
methods such as random forests or GBMs using 
bootstrap procedures [e.g. @hastie2009; @finnegan2015].
Furthermore, uncertainty from the component models could 
be included in superensembles.
These superensembles could be fit using any errors-in-variables or
measurement-error modelling approach [e.g. @carroll2006]

Multi-model inference in the form of coefficient averaging weighted by
information theoretics such as the Akaike Information Criterion (AIC) is
a common analytical approach in fisheries and ecology [e.g., @burnham2002; @johnson2004;
@grueber2011]. The ensemble methods described in this paper share similarities
with coefficient averaging but differ in other important ways. Ensemble methods
and coefficient averaging share the long-held notion that multiple working
hypotheses can contribute useful information for inference [@chamberlin1890].
A fundamental difference is that coefficient averaging focuses on averaging
*coefficients* whereas ensembles instead average *predictions*. Thus, ensembles
provide a general purpose tool: they do not require information theoretics
and they can combine different types of models (e.g., parametric and
non-parametric models or frequentist and Bayesian predictions). Furthermore,
superensembles extend these benefits by allowing model predictions to be
combined via non-linear functions that are tuned to known data.

A strength of superensembles is that they can be tailored to predict specific
response variables. For example, we built separate superensemble models of mean
$B/B_\mathrm{MSY}$ and the slope of $B/B_\mathrm{MSY}$. The same set of model
weights or non-linear relationships need not hold across different response
variables. For instance, SSCOM contributed little to the GBM superensemble
estimate of status at higher levels of predicted $B/B_\mathrm{MSY}$ (Fig.
\ref{partial-sim}), but contributed strongly to estimates of trend (Fig.
\ref{partial-sim-slope}). Formally, fitting superensemble models to specific
quantities of interest (such as the slope of $B/B_\mathrm{MSY}$) provides an additional
calibration step to a quantity of interest [@rykiel1996].\R{A10}
This ensemble calibration could include a loss function
tailored to the goals of the model, say placing greater weight on accuracy at
lower rather than higher status levels. Conversely, because superensembles are
tailored to a specific response and loss function, superensembles force a
modeller to choose an operational purpose for their model upfront [*sensu*
@dickey-collas2014]. \R{E}For instance, one could have an ensemble 
estimate of $B$ and an ensemble estimate of $B_0$, but their
ratio may not be the same as an ensemble estimate of $B/B_0$.
A modeller might therefore choose to focus on $B/B_0$, 
which provides a unitless ratio, is easier to compare across stocks, 
and the ratio is often a more stable estimate across models [@deroba2015].

As @box1987 noted, all models are wrong, but some may still be useful. The
ensemble methods we investigated attempt to piece together the useful parts of
candidate models to build a model with improved performance. Instead of viewing
the superensemble as a black box, we think considerable mechanistic
understanding can be gained by studying its structure. For example, when SSCOM
estimates low status this is likely the case, conversely when COMSIR
estimates low status, the true status is more likely to be high (Fig.
\ref{partial-sim}). These models have two main differences: (1) the form of
effort dynamics and (2) the allowance for both measurement and process error in
SSCOM, whereas the implemented COMSIR admits measurement error only. Were the
methods to differ only in effort dynamics, the results point towards a more
suitable representation of effort dynamics at low biomasses in SSCOM. We think
that such investigation of the structure of a superensemble may lead to
improvement in the mechanisms assumed in individual models.

Combining predictions from multiple models via superensemble methods is broadly
useful in other subfields of fisheries science and ecology in general.
\R{A}In fisheries science, superensembles provide an 
additional tool to assist with some longstanding issues.
For example, superensembles are helpful since modelers need not 
decide on one model ---
instead of deciding on dome versus asymptotic fisheries 
selectivity [e.g., @sampson2012], 
or on whether to fix or estimate natural mortality [e.g., @johnson2014],
superensembles can use multiple models to draw inference.
Furthermore, the relative contributions of individual models can help tease apart 
the conditions under which various model assumptions 
result in the most accurate predictions.
Finally, superensembles can be used to directly 
estimate other quantities of interest in fisheries science. 
For instance, superensembles could help assess overfishing by estimating 
fishing mortality compared to fishing mortality at MSY ($F/F_\mathrm{MSY}$)
or be trained to estimate natural mortality.

More broadly, in ecology, 
predictions about extinction risk are widely used at national (e.g., the US
Endangered Species Act and the Canadian Species at Risk Act) and international
[e.g., the IUCN Red List, @iucn2015] levels. These risk assessments generally
involve fitting regression models to outcomes for individual species along with
predictors of extinction risk [e.g., @anderson2011a; @pinsky2011], or fitting
population-dynamic models to data for individual species [e.g., @dfo2010]. Both
types of models are prone to error caused by model-misspecification and
therefore results are sensitive to decisions about model structure
[@brooks2015]. Although there are options to account for potential
model-misspecification in determination of species risk [e.g., coefficient
averaging, @burnham2002; generalized modeling, @yeakel2011; or semi-parametric
methods, @thorson2014], ensemble methods are a relatively simple way to combine
predictions in a transparent manner. Beyond estimates of status and trend,
ensemble methods could be used, for example, to increase the robustness of
spatial predictions when designing networks of protected areas
[@rassweiler2014] or to forecast potential spatial shifts in species
distribution given climate impacts [@harsch2014].
In any case, superensembles are not a panacea 
and are ultimately limited by the quality, breadth, and representativeness
of simulated or trusted data to which they are calibrated.

\noindent

# Acknowledgements

We thank members of Phase I of the working group "Developing new approaches to
global stock status assessment and fishery production potential of the seas"
who contributed to developing the data-limited methods and simulations used in
our analysis. We thank E. Jardim, F. Scott, and J.A. Hutchings for helpful
comments during the development of this project, and R.D. Methot for comments
on an earlier version of the manuscript. We thank
the Gordon and Betty Moore Foundation for funding the working group "Applying
data-limited stock status models and developing management guidance for
unassessed fish stocks".
