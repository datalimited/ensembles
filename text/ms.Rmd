```{r, echo=FALSE}
# Values to use in the paper:
load("values.rda") # generated by `../analysis/9-output-values.R`

# Includes: mean_sim, mean_ram, slope_sim, auc_sim, ram_stocks_n

# mean_sim, mean_ram, and slope_sim have the following list elements:
# > names(mean_sim)
# "mach_vs_ind_mare_fold" "ind_corr_range" "mach_corr_range" "ind_mare_range"  
# "mach_mare_range" "ensemble_mre_range" "ind_mre_range"
 
# auc_sim has the following list elements:
# > names(auc_sim)
# "mach_range" "ind_range" 
```

\noindent
Running head: Superensembles of population status

\bigskip

\begin{Large}
\noindent
Improving estimates of population status and trajectory with superensemble models
\end{Large}

\bigskip

\noindent
Sean C. Anderson^1\*^, 
Jamie Afflerbach^2^, 
Mark Dickey-Collas^3^, 
Olaf P. Jensen^4^, 
Kristin M. Kleisner^5^, 
Catherine Longo^2^, 
C\'{o}il\'{i}n Minto^6^, 
Giacomo Chato Osio^7^, 
Dan Ovando^8^, 
Andrew A. Rosenberg^9^, 
Elizabeth R. Selig^10^, 
James T. Thorson^11^,
Jessica C. Walsh^1^,
Andrew B. Cooper^1^
*(order to be determined and others may be added)*

\bigskip

\noindent
^1^School of Resource and Environmental Management,
Simon Fraser University, Burnaby, BC, V5A 1S6, Canada

\noindent
^2^National Center for Ecological Analysis and Synthesis

\noindent
^3^International Council for the Exploration of the Sea

\noindent
^5^NOAA/NMFS/NEFSC, Ecosystem Assessment Program, 
Woods Hole, MA, 02543, USA

\noindent
^6^Galway-Mayo Institute of Technology, 
Marine and Freshwater Research Centre, Galway, 00000, Ireland

\noindent
^7^EC JRC, IPSC, MAU, Italy

\noindent
^8^University of California Santa Barbara, 
Bren School of Environmental Science and Management, 
Santa Barbara, CA, 93106-5131, USA

\noindent
^9^Union of Concerned Scientists, Cambridge, MA, USA

\noindent
^10^Conservation International, Arlington, VA, USA

\noindent
^11^National Marine Fisheries Service, 
National Oceanic and Atmospheric Administration, 
Fisheries Resource Assessment and Monitoring Division, 
Northwest Fisheries Science Center, Seattle, WA, 98112, USA

\noindent
^\*^Corresponding author: Sean C. Anderson,
School of Resource and Environmental Management,
Simon Fraser University,
Burnaby BC, V5A 1S6;
E-mail: sean_anderson@sfu.ca

\clearpage

# Abstract

<!--up to 350 words-->

Ecological resource managers are often faced with reconciling multiple potentially
conflicting estimates of population status and trajectory.  One solution is to
average these predictions in an ensemble, but this approach ignores covariance
between models and may not optimally reduce bias and improve accuracy.
Superensemble models, commonly used in the fields of climate and weather
science, may provide a superior solution. Superensembles use the predictions
from multiple models as covariates in a new statistical model. Here we evaluate
the potential for ensemble and superensemble methods to improve estimates of fish population
status and trajectory. We fit four models of population and exploitation
dynamics to data-limited populations and combine their estimates of the mean and
slope of biomass at maximum sustainable yield ($B/B_\mathrm{MSY}$) with an
ensemble average and three superensembles: a linear model, a random forest, and
a boosted regression tree. We build our models on a simulated dataset of 5760
fish stocks and test them with cross-validation and a global database of 
`r ram_stocks_n` stock assessments. 
We find that ensemble and superensemble models
substantially improve estimates of population status and trajectory.
Machine-learning superensembles performed the best for population status: accuracy improved
`r mean_sim$mach_vs_ind_mare_fold`%, correlation between predicted and true
status improved from 
`r mean_sim$ind_corr_range` to `r mean_sim$mach_corr_range`, and bias (median
proportional error) declined from 
`r mean_sim$ind_mre_range` to `r mean_sim$mach_mre_range`. 
We found similar improvements when predicting the trajectory of status. When the
simulation-trained ensembles were applied to global fish stocks, the best
ensembles still improved all dimensions of performance over the next-best
individual model. Ensemble and superensemble models have the potential to
improve estimates of population status and trajectory across many taxa beyond
fish. However, they must be carefully tested, formed from a diverse set of
accurate models, and built on a dataset sufficiently representative of the
populations they are applied to.

<!-- up to 12 keywords -->
\noindent 
Keywords: data-limited fisheries, ensemble methods, model averaging, population dynamics, sustainable resource management

# Introduction

Despite considerable advancements in the systematic collection of ecological
population data, we still have little data with which to estimate population
status for most species (IUCN, FAO, amphibian citations etc.). Yet, a large
portion of these data-limited populations face human-caused pressures and may
warrant conservation action (IUCN REF). This scenario calls for methods to
assess the status of populations that rely on limited sources of data. While,
such methods are increasingly available (e.g. REF, REF), their application faces
two key challenges: (1) performance is relatively poor compared to more
data-rich forms of population modelling (e.g. REF) and (2) resource managers are
often faced with reconciling multiple potentially conflicting estimates of
population status (e.g. REF).

One solution is to take the average or weighted average of an ensemble of model
predictions. Such ensembles are typically more accurate and less biased
than individual model estimates and can integrate uncertainty in model
structure, initial conditions, and parameter estimation [@araujo2007]. For
instance, REF-international did this and REF-national did this.

Whereas averages or weighted averages can improve predictions, they may not
optimally leverage available data. Some models perform better than others in
certain conditions and the covariance between model predictions may contain
information that can both improve predictions and aid understanding of
individual model performance. We can exploit these characteristics by using the
predictions from a group of models as inputs into a separate statistical model.
This technique, sometimes called 'superensemble' modelling, is common in climate
and weather forecasting. The superensemble is fit to some training dataset where
outcomes are well known and used to predict on a separate dataset. For example,
@krishnamurti1999 combined predictions of wind and precipitation in Asian
monsoons via a superensemble regression fit to observed data. Their
superensemble considerably outperformed any individual prediction or weighted
average of predictions in terms of accuracy. 

<!-- Ensemble methods, which combine the output from multiple models as
predictors in a new 'ensemble' model, are one possible solution to this
problem. A simple ensemble model might take the average of multiple model
predictions, perhaps weighted by some goodness of fit. A complex ensemble model
might also take into account non-linear interactions between model predictions
and additional covariates. Ensemble models are popular in other fields
including public health [@thomson2006], agriculture [@cantelaube2005], climate
science [@murphy2004; @tebaldi2007; @pierce2009], and machine learning
[@dietterich2000].-->

In fisheries science, a common task is estimating the status and trajectory of
an exploited fish population. For the majority of fish stocks, we have limited
fisheries independent data to estimate stock status [@fao2014]. In recent
years, a number of methods have been proposed to derive population status based
on the limited information that is broadly available: fisheries catch time
series and basic knowledge about the productivity of the species 
[e.g. @vasconcellos2005; @martell2013]. Recently, @rosenberg2014
investigated the ability for four such models to estimate population status through
a large-scale simulation experiment. A key finding was that these models
frequently disagree on population status and any one model has relatively poor
predictive ability on average (e.g. Fig. \ref{motivate}).

Here, we estimate population status and trajectory of exploited fish populations
using ensemble and superensemble models (collectively referred to 'ensemble
methods'). We explore a variety of approaches applied to both simulated and
real-world fish stocks and compare predictive performance. We find that
ensembles, and in particular superensembles, generally improved predictive
performance of status and trajectory of fish populations over any single
model---especially when considered across multiple dimensions of predictive
ability and across multiple datasets. Furthermore, the output from
superensembles can aid mechanistic understanding of how individual models of
status and trajectory perform.

# Methods

<!--
We tested the ability of ensemble models to improve estimates of population
status and trajectory when applied to both a large simulated dataset and a
global database of assessed stock status. Here we describe the datasets,
individual models of population status, and ensemble methods to combine those
estimates. We then describe how we evaluated the ability of the various models
to estimate population status.
-->

<!--
## Datasets
-->

We first developed and tested ensemble methods with a fully factorial simulated
dataset [@rosenberg2014]. These previously published simulation models included
three life histories: small pelagic, demersal, large pelagic; three levels of
how depleted biomass was at the start of the dataset compared to carrying
capacity: 100%, 70%, and 40% of carrying capacity; and four exploitation
dynamics: a constant exploitation rate, a exploitation rate that is coupled
with biomass to mimic an open-access single-species fishery, a scenario where
exploitation rate increases to a fixed level, and a 'roller-coaster' scenario
where the exploitation rate increases and then decreases. Process noise was
introduced to the models through two levels of multiplicative recruitment
variability: $N(0, 0.2)$ and $N(0, 0.6)$, and that process noise was either
uncorrelated or had a first-order autoregressive correlation of 0.6. The
simulation included a scenario without observation error and with
multiplicative observation error around catch at $N(0, 0.2)$. 

@rosenberg2014 ran ten iterations for each combination of
factors adding stochastic draws of recruitment and catch-recording variability
each time to generate a total of 5760 stocks. The simulation models were built
in the package `FLR` [@kell2007] for the statistical software \textsf{R}
[@r2015]. The code to generate the simulations is available at
<https://github.com/flr/StockSims> (TODO: not there). 

We also tested our ensemble methods on the RAM Legacy Stock Assessment Database
[@ricard2012]. Our analysis of the stock-assessment database was based on
version XX downloaded on XX. After removing stocks for which the models
described below did not converge, this database included `r ram_stocks_n`
stocks.

## Individual models of population status

We fit four individual data-limited models to estimate $B/B_\mathrm{MSY}$. Three of the
models are mechanistic and based generally on Schaefer biomass dynamics
[@schaefer1954] of the form

$$\hat{B}_{t+1} = B_t + r B_t \left(1 - B_t / B_0 \right) - C_t,$$

\noindent
where $\hat{B}_{t+1}$ represents predicted biomass at time $t$ plus one year,
$B_t$ represents biomass at time $t$, $r$ represents the intrinsic population
growth rate, $B_0$ represents unfished biomass or carrying capacity, and $C$
represents catch. @rosenberg2014 provide a full summary of
these four methods and code to fit all the models is available in an
accompanying package `datalimited` for the statistical software \textsf{R}. In
summary:

* *CMSY* (catch-MSY) implements a stock-reduction analysis with Schaefer
  biomass dynamics [@martell2013]. It requires a prior distribution for $r$ and
  assigns a prior to the relative proportion of biomass at the end compared to
  unfished biomass (depletion) based on the percentage of maximum catch at the
  end of the time series.

* *COM-SIR* (catch-only-model with sample-importance-resampling) is a coupled
  harvest-dynamics model [@vasconcellos2005]. Biomass is assumed to follow a
  Schaefer model and harvest dynamics are assumed to follow a logistic model.
  The model is fit with a sample-importance-sampling algorithm [@rosenberg2014].

* *SSCOM* (state-space catch-only model) is a hierarchical model that, similar
  to COM-SIR, is based on a coupled harvest-dynamics model [@thorson2013].
  SSCOM estimates unobserved dynamics in both fishing effort and the fished
  population based on a catch time series and priors on $r$, the maximum rate
  of increase of fishing effort, and the magnitude of various sources of
  stochasticity. The model is fit in a Bayesian state-space framework to
  integrate across three forms of stochasticity: variation in effort,
  population dynamics, and fishing efficiency [@thorson2013].

* *mPRM* (modified panel regression model) is a modified version of the
  panel-regression model from @costello2012. Unlike the other
  models, mPRM is empirical and not mechanistic---it uses the RAM Legacy Stock
  Assessment database to fit a regression model to a series of characteristics
  of the catch time series and stock with stock-assessed $B/B_\mathrm{MSY}$ as
  the response. The model used in this paper is modified from the original in
  that it condenses the life-history categories into three categories to match
  the simulated dataset, removes the maximum catch predictor since the absolute
  catch in the simulated dataset is arbitrary, and does not implement the bias
  correction needed in @costello2012 for deriving aggregate
  estimates of mean status across multiple stocks.

  <!--TODO: build in fig:didactic -->

## Additional covariates

Superensemble models allow us to incorporate additional covariates 
and potentially leverage interactions between these covariates and individual
model predictions. Additional covariates could be, for example, life-history
characteristics, information on exploitation patterns, or statistical
properties of the data. For simplicity, and to allow us to apply models
developed with the simulated dataset to the real-world dataset, we added only one
set of additional covariates: spectral properties of the catch time series.
Spectral analysis decomposes a time series into the frequency domain and
provides a means of describing the statistical properties of the catch series
that is independent of time series length and absolute magnitude of catch. We fit
spectral models to the scaled catch time series (catch divided by maximum
catch) with the `spec.ar` function in \textsf{R} and recorded representative
short- and long-term spectral densities at frequencies of 0.20 and 0.05, which
correspond to 5- and 20-year cycles.

## Superensemble models

The individual models we seek to combine with superensembles provide time
series of $B/B_\mathrm{MSY}$. Therefore, we can use superensembles to estimate
multiple properties of these status time series. Here, we focus on two
properties: the mean and slope of $B/B_\mathrm{MSY}$ in the last 5 years.
Together, these quantities address the recent state and trajectory of status,
which are both of management and conservation interest [e.g. @mace2008]. To
avoid undue influence of the end points of the time series on the calculated
slope, we measured the slope as the Theil-Sen estimator of median slope
[@theil1950].

Here we compare an ensemble average and three superensembles of varying
complexity: a linear model with two-way interactions, a random forest, and
boosted regression tree. The general form of our models was

$$
\hat{\theta} = 
\argmin_\theta \left( L(b \, | \, \hat{b}_{\mathrm{ensemble}}) \right)
$$
$$
\hat{b}_\mathrm{ensemble} = 
f \left( \hat{b}_\mathrm{CMSY}, \, \hat{b}_\mathrm{COM-SIR}, \,
\hat{b}_\mathrm{SSCOM}, \, \hat{b}_\mathrm{mPRM}, \,
S(0.2), \, S(0.5), \, | \, \theta \right)
$$

\noindent 
where $L$ is a generalized loss function, $f$ is some generalized regression
function, $\hat{b}_\mathrm{CMSY}$ is the mean or slope prediction from the CMSY
model etc. (Table \ref{tab:predictors}), $S(0.2)$ and $S(0.05)$ represent the
spectral density of the scaled catch series at frequencies of 0.20 and 0.05, $b$
is the mean or slope value the model is fitted to, and $\theta$ is the set of
parameters for the regression function. For all superensemble models of mean
$B/B_\mathrm{MSY}$---a ratio bounded at zero---we fit the models in log space
and exponentiated the predictions. For the estimates of $B/B_\mathrm{MSY}$
slope, which are not bounded at zero, we fit superensemble models on the natural
untransformed scale.

We fit the linear model ensemble with second-order interactions. We fit two
machine learning ensemble models based on regression trees. Regression trees
sequentially determine what value of a predictor best splits the response data
into two 'branches' based on a specified loss function [@breiman1984]. In random
forests, a series of regression trees are built on a random subset of the data
and random subset of the covariates of the model [@breiman2001]. In generalized
boosted models (GBMs), each subsequent model is fit to the residuals from the
previous model; data points that are fit poorly in a given model are given more
weight in the next model [@elith2008]. Random forests and GBMs can provide
strong predictive performance and fit highly non-linear relationships
[@elith2008; @hastie2009]. We fit random forest models with the `randomForest`
package [@liaw2002] for \textsf{R} with the default argument values. We fit
boosted regression tree models with the `gbm` package [@ridgeway2015] for
\textsf{R}. Based on cross-validation with the **caret** \textsf{R} package, we
fit GBMs with 2000 trees, an interaction depth of $6$, a learning rate
(shrinkage parameter) of $0.01$, and all other arguments at their default
values.                                                

## Testing model performance

A critical component to any predictive modelling exercise is to evaluate the
performance of a model on new data [@hastie2009]. We used repeated three-fold
cross validation to test predictive performance: we randomly divided the
dataset into three sets, built ensemble models on two-thirds of the data, and
evaluated predictive performance on the remaining third. We repeated this
across each of the three splits and then repeated the whole procedure 50 times
to account for bias that may result from any one set of validation splits.
Since the dynamics of simulated populations differing only in the stochastic
draw of random values are likely similar, we grouped these stocks (10 per
factorial cell) in the cross-validation process into either the training or
testing split. Since mPRM is built on the RAM Legacy Stock Assessment database,
when testing model performance on the stock assessment database, we refit mPRM
on each training split.

Predictive performance can be evaluated with metrics that represent a variety
of modelling goals. For continuous response variables such as the mean and
slope of population status, performance metrics often measure some form of
bias, precision, accuracy (a combination of bias and precision), or the ability
to correctly rank or correlate across populations [e.g. @walther2005]. Here,
we measure proportional error, defined as
$(\hat{b} - b)/b$, where $\hat{b}$ and $b$ represent estimated and 'true'
(or stock-assessed) mean or slope of $B/B_\mathrm{MSY}$. We calculated median proportional
error to measure bias, median absolute proportional error to measure accuracy,
and Spearman's rank-order correlation to measure the ability to correctly rank
populations.

# Results

Applied to the simulated dataset of known status, the individual models had
variable success at recovering the mean (status) and slope (trajectory) of
$B/B_\mathrm{MSY}$ in the last five years. All models exhibited a high degree of scatter
around the 1:1 line of perfect status prediction (Fig. \ref{hexagon}). CMSY
exhibited bimodal predictions of status but had the best rank-order correlation
and accuracy scores (Fig. \ref{performance}a). COM-SIR and SSCOM both correctly
identified a number of low status stocks, but frequently predicted a high
status when status was in fact low (Fig. \ref{hexagon}b, c). mPRM had
relatively poor ability to predict status for the simulated dataset (Fig.
\ref{hexagon}d). There was generally little correlation between true and
predicted trajectory for any of the individual models besides SSCOM (Figs
\ref{scatter-sim-slope}a--d).

Ensemble models, and in particular the machine learning ensemble models (random
forest and GBM), generally improved estimates of status and trajectory over any individual model (Fig.
\ref{hexagon}e--h, Fig. \ref{scatter-sim-slope}e--h). Compared to the
individual models, machine learning ensembles improved accuracy (median
absolute proportional error) by 
`r mean_sim$mach_vs_ind_mare_fold`%, 
increased rank-order correlation from `r mean_sim$ind_corr_range` 
to `r mean_sim$mach_corr_range`, 
and reduced bias (median proportional error) from 
`r mean_sim$ind_mre_range` to 
`r mean_sim$mach_mre_range` (Fig. \ref{performance}a). 
These ensembles also had better ability to distinguish if
simulated stocks were above or below $B/B_\mathrm{MSY} = 1$ (Fig.
\ref{roc-sim}). Results were similar when predicting trajectory: compared to
individual models, machine learning ensembles improved accuracy by 
`r slope_sim$mach_vs_ind_mare_fold`%, increased rank-order correlation from 
`r slope_sim$ind_corr_range` to 
`r slope_sim$mach_corr_range`, and reduced bias
from `r slope_sim$ind_mre_range` 
to `r slope_sim$mach_mre_range` (Fig. \ref{performance-sim-slope}). The
ensemble models that simply took a mean of the individual models ranked
slightly behind the best individual model for estimating fish stock status (CMSY; Fig.
\ref{performance}a) and had slightly lower correlation but higher accuracy than
the best individual model at predicting the trends of status (SSCOM; Fig.
\ref{performance-sim-slope}).

<!--TODO: check labels on Figure S4 â€“ repeat of (d) and missing (f)-->
The ensemble models were able to improve the predictions of status by
exploiting the best properties of individual models, the covariance between
individual models, and interactions with other covariates. For example, SSCOM
had strong predictive ability when it predicted low $B/B_\mathrm{MSY}$ (Fig.
\ref{hexagon}c, Fig. \ref{partial-sim}c) and CMSY predictions were
approximately linearly related to $B/B_\mathrm{MSY}$ within the low and high prediction
clusters (Fig. \ref{partial-sim}). Ensembles also exploited the covariance
between individual model predictions. For instance, both the linear model and
GBM ensemble suggest that if mPRM and SSCOM predict high status, the true
status also tends to be high (Figs \ref{lm-coefs}, \ref{partial-2d-sim}l). The
addition of spectral density covariates helped the ensemble models correctly
predict higher status values (Fig. \ref{hexagon}g, h) by exploiting
interactions between the spectral density predictors and the individual model
predictions (Fig. \ref{partial-2d-sim}). Although the additional covariates
improved ensemble fit, performance of the ensembles was only marginally
degraded by removing these covariates (Fig. \ref{hexagon} vs. Fig.
\ref{hexagon-sim-basic}).

When applied to the stock assessment database, the ensemble models---trained
exclusively on the simulated dataset---generally performed as well or better
than the best individual models. The mean, random forest, and GBM ensembles
even outperformed the mPRM method which is trained directly on the RAM Legacy Stock
Assessment database itself (Fig. \ref{performance}b, Fig. \ref{hexagon-ram}).
Compared to the individual models, the machine learning ensembles increased
accuracy by `r mean_ram$mach_vs_ind_mare_fold`%, 
improved correlation from `r mean_ram$ind_corr_range` to 
`r mean_ram$mach_corr_range`, and reduced bias from
`r mean_ram$ind_mre_range` to `r mean_ram$mach_mre_range`.                    

<!--
Some individual models performed nearly as well as the ensembles in one or two performance dimensions: e.g. SSCOM does nearly as well as the ensembles at predicting the slope of $B/B_\mathrm{MSY}$ (Fig. \ref{performance-sim-slope}). But no individual method is consistently nearly as good as the ensembles. SSCOM does more poorly than the ensembles and other individual models in terms of accuracy, rank-order correlation, and bias at estimating the mean $B/B_\mathrm{MSY}$ for both the simulated and stock-assessment datasets (Fig. \ref{performance}).
-->

# Discussion

Ensemble methods provide a useful approach to situations where conservation and
ecological resource management decisions must be made on the basis of multiple,
potentially contrasting estimates of status---a situation common to many
ecological settings beyond fisheries. Compared to individual models of fish
population status, ensemble methods were consistently the best or among the best
across three performance dimensions (accuracy, bias, and rank-order
performance), two response variables (status and trajectory), two datasets
(simulated and global fisheries), and multiple ensemble methods (from a
simple average to machine learning models). Our results suggest choosing an
superensemble model that allows for non-parametric relationships, such as machine
learning methods. These methods provide added insight into individual model
behaviour and generally performed the best, however even a simple average of
predictions across multiple models may be useful.

Certain conditions will make some ensemble models more effective than others.
First, ensembles will be most effective when they are comprised of diverse
individual models that choose different structural model forms, explore
contrasting but plausible ranges of parameter values, or make errors in
uncorrelated ways [@ali1996; @dietterich2000; @tebaldi2007]. Such
individual models would be expected to perform well in different conditions and
an ensemble model can exploit the best predictive performance of each. Second,
ensemble models will be most effective when they are not overfit to the
training dataset. Cross-validation testing [@caruana2004; @hastie2009] and
methods that are robust to over-fitting such as random forests [@breiman2001],
may help avoid overfitting ensemble models. We note that our simplest ensemble
model, an average of individual model predictions, performed approximately as
well as complex machine learning models when we applied our ensembles to an
entirely new dataset (Fig. \ref{performance}b) and averages are a common
ensemble approach in climate science [@ipcc2013]. Third, ensemble
models will be most effective when they are trained on data that are
representative of the dataset of interest [@knutti2009; @weigel2010].
Cross-validation within a training dataset will provide an optimistically biased
impression of predictive performance if the training dataset fundamentally
differs from the dataset of interest [@hastie2009].

Multi-model inference in the form of coefficient averaging weighted by
information theoretics such as the Akaike Information Criteron (AIC) is a
common analytical approach in ecology [@burnham2002; @johnson2004;
@grueber2011]. The ensemble methods described in this paper share similarities
with coefficient averaging but differ in other important ways. Ensemble models
and coefficient averaging share the long-held notion that multiple working
hypotheses can contribute useful information for inference [@chamberlin1890]. A
fundamental difference is that coefficient averaging focuses on averaging
*coefficients* whereas ensembles instead average *predictions*. By focussing on
predictions, ensembles do not suffer from the muddled inference of coefficient
averaging when coefficients covary [@cade2015] and are a more general purpose
tool: they do not require information theoretics, they can combine different
types of models (e.g. parametric and non-parametric models or frequentist and
Bayesian predictions), and they can combine predictions in complex non-linear
ways that may depend on additional covariates.

A strength of superensembles is that they can be tailored to specific response
variables. For example, we built separate ensemble models of mean
$B/B_\mathrm{MSY}$ and the slope of $B/B_\mathrm{MSY}$. The same set of model
weights or non-linear relationships need not hold across different response
variables. For example, SSCOM contributed little to the GBM ensemble estimate
of status at higher levels of predicted $B/B_\mathrm{MSY}$, but contributed
strongly to estimates of slope (TODO FIG). Formally, fitting ensemble models to
specific quantities of interest provides an additional calibration step
[@rykiel1996]. Individual models are typically calibrated (i.e. parameters are
estimated) based on a single response variable; however, this model might then
provide a biased or inaccurate estimate of other quantities of interest. For
example, a data-limited fisheries model might calibrate parameters by
maximizing likelihood with respect to observed and predicted catches but those
parameters may then provide a biased or inaccurate estimate of the mean and
slope of $B/B_\mathrm{MSY}$. Ensemble methods provide a final calibration step
to a quantity of interest. This ensemble calibration could further include a
loss function tailored to the goals of the model, say placing greater weight on
accuracy at low status levels than higher values.

* Coilin: How ensembles can help understand mechanistic underpinnings of individual 
  models
    * discover surprises: counterintuitive relationships and interactions
    * can dig into surprising conditions and learn about why certain models fail; 
      potentially learn how to improve models
    * can learn under what conditions certain models perform well
    * pick out an example

Combining predictions from multiple models via superensemble methods is also
broadly useful in other subfields of ecology and fisheries science. Predictions
about extinction risk are widely used at the national (e.g. the US Endangered
Species Act and the Canadian Species at Risk Act) and international (e.g. the
IUCN Red List, [@iucn2015]) levels. These risk assessments generally involve
fitting regression models to outcomes for individual species along with
predictors of species risk [e.g. @anderson2011a; @pinsky2011] (*TODO instead
get references from official national/international risk assessments?*), or
population dynamics models to data for individual species [e.g. @dfo2010].
Both types of models are prone to error caused by model-misspecification and
therefore results are sensitive to decisions about model structure
[@brooks2015]. Although there are options to account for potential
model-misspecification in determination of species risk (e.g., coefficient
averaging, [@burnham2002], generalized modeling [@yeakel2011], or
semi-parametric methods, [@thorson2014]), ensemble methods are a relatively
simple way to combine predictions in a transparent manner. Beyond estimates of
status and trajectory, ensemble methods could be used to increase the robustness
of spatial predictions when designing networks of protected areas
[@rassweiler2014], or to forecast potential spatial shifts in species
distribution given climate impacts [@harsch2014].
	
<!--
* Other possible topics:
    * how much better are the ensembles really? translate the results into
      some concrete examples in an absolute not relative sense
    * other types of ensembles: assigning weights, incorporating uncertainty
      around estimates, GAMs, ...
    * when wouldn't you want to use ensembles? perhaps if models represent 
      dichotomous assumptions where either one or the other is right and imply
      different management actions? ...
    * your ideas here...

Why do ensembles work?

- 'representational' no one model can contain all 'hypothetical functional
  forms'... many models can cover many hypotheses @dietterich2000
- error cancellation --- especially if multiple models make errors in
  uncorrelated ways [@ali1996]
- they provide an additional step of 'calibration' [@rykiel1996] where the
  predictions are calibrated to data of known or assumed truth about a
  particular aspect of interest in the data (e.g. mean status, slope of status)
  whereas individual model may be calibrated (parameters estimated) with a
  slightly different response in mind (e.g. maximizing likelihood with respect
  to observed and predicted catches)
- we usually do not know which individual model is 'best' [@hagedorn2005];
  ensembles have the property of rarely being far from the best

-->

# Authors' contributions

TODO *Example: AB carried out the molecular lab work, participated in data
analysis, carried out sequence alignments, participated in the design of the
study and drafted the manuscript; CD carried out the statistical analyses; EF
collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final
approval for publication.*

# Acknowledgements

TODO

# Funding

TODO

<!--Funding...-->

<!--# Citation notes-->

<!--
- @breiner2015 ensemble models of species distribution models for rare species

- @jones2015 ensemble models of species distribution models - globally for
  marine biodiversity

- [@greene2006] example similar to ours but with climate models (Bayesian
  multilevel ensemble)

- [@kell2007] FLR

- multiple learners book [@alpaydin2010]

- [@caruana2004] nice paper on ensemble models; prob of overfitting increases
  with more models, bagging important

- [@tebaldi2007] key paper: review of 'multi-model ensembles for climate projections'

- famous ensemble is DEMETER: Development of a European Multi-model Ensemble
  System for Seasonal to Interannual Prediction @hagedorn2005 is a good
  reference

- Examples of where ensemble models are shown to be better than any one:
  @thomson2006 (public health - malaria), @cantelaube2005 (agriculture - crop
  yield) (citations taken from @tebaldi2007)

- simple averages are used very commonly in climate science - e.g. IPCC 2001

- @tebaldi2007: weighting obviously makes sense, but how do we define a
  performance metric that is based on past observations that is relevant to the
  future? 

- @tebaldi2007: model independence important

- for climate, weighted averages perform better than simple averages [@min2006],
  but are those same weights applicable to the future (or in our case other
  fisheries)?

Why do ensemble methods work?

- error cancellation is one but not the only reason for superiority of ensemble
  models [@hagedorn2005]

- ensemble model may be only marginally better than the best single model in
  any given case, but we don't usually know which is the best single model
  [@hagedorn2005]

- ensemble models useful for regional climate models too; as an example,
  @pierce2009 use 42 metrics to characterize model performance in regional
  climate model ensembles; found that ensemble models were superior to any one
  model --- especially when considering multiple metrics

- @knutti2009: key review paper on motivation and challenges of combining
  climate projection models; performance on testing / current data may only
  weakly relate to future / other datasets

- @murphy2004: Nature paper on ensembles of climate model simulations; weighted
  average better performance than unweighted average

- @dietterich2000: highly cited book chapter "Ensemble Methods in Machine
  Learning"; ensembles are often much more accurate than the "individual
  classifiers that make them up"; but components must be diverse and accurate

- @dietterich2000: a main justification for ensembles: they are
  representational --- no one model usually can contain all hypothetical
  functional forms, but many separate models can cover more hypotheses

- strong paper showing that ensemble models are most accurate when the various
  individual models make errors in uncorrelated ways [@ali1996]

- without substantial training-testing data, appropriate model weights can be
  very hard to deduce and can cause more harm than good (compared to equal
  weighting) [@weigel2010] (they give the example of seasonal forecasting where
  a ton of hind cast testing can be done, vs. long-term climate) (asymmetrical
  loss function)

- [@weigel2010] optimal weights are always as good or better than equal
  weights, but if you get the weights wrong, you can be better off just using
  equal weights; but averaging was almost always better than any one model

- @rykiel1996 "Testing ecological models: the meaning of validation" (see for
  performance criteria, theory on model testing and assessment)

model averaging AIC:
f Chamberlin 1890).  multiple woring hypotheses
  (e.g., Johnson and Omland 2004, Hobbs and Hilborn 2006, Burnham et al. 2011,
  52 Grueber et al. 2011). 
-->

